{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt-get update\n",
    "# sudo apt install chromium-chromedriver\n",
    "# sudo apt install chromium-bsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "# import pandas as pd\n",
    "# from sqlalchemy.util import dataclass_fields\n",
    "\n",
    "# from urllib.parse import urljoin\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "# from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import sqlite3\n",
    "from fastcore.basics import patch_to\n",
    "\n",
    "# download_url\n",
    "import time\n",
    "import selenium\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "\n",
    "    sqlite_driver_db: str\n",
    "    sqlit_con: sqlite3.connect\n",
    "\n",
    "    url_to_visit_ls: list[str]\n",
    "\n",
    "    url_visited_ls: list[str] = []\n",
    "    article_ls : list[str] = []\n",
    "\n",
    "    def __init__(self,\n",
    "                 sqlite_driver_db: str,\n",
    "                 url_to_visit_ls: list[str] = [],\n",
    "                 base_url: str = None):\n",
    "\n",
    "        self.sqlite_driver_db = sqlite_driver_db\n",
    "        self.sqlit_con = sqlite3.connect(self.sqlite_driver_db)\n",
    "        self.base_url = base_url\n",
    "        self.url_to_visit_ls = url_to_visit_ls\n",
    "\n",
    "\n",
    "    \n",
    "    def _add_url_to_visit(self, url ):\n",
    "      if url not in self.url_visited_ls and url not in self.url_to_visit_ls:\n",
    "        self.url_to_visit_ls.append(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "cd = Crawler(sqlite_driver_db = 'kb_scrape', base_url = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(Crawler)\n",
    "def run(self: Crawler, debug_prn: bool = False):\n",
    "    \n",
    "    counter = 0\n",
    "    upload_counter = 0\n",
    "    num_upload = 20\n",
    "    offset = 0\n",
    "    max_upload = 10000\n",
    "\n",
    "    while self.url_to_visit_ls and counter <= max_upload:\n",
    "        url = self.url_to_visit_ls.pop(0)\n",
    "        \n",
    "        logging.info(f'Crawling: {url}')\n",
    "        \n",
    "        try:\n",
    "            # self.crawl(url, debug_prn)\n",
    "\n",
    "            pass\n",
    "            \n",
    "        except Exception:\n",
    "            logging.exception(f'Failed to crawl: {url}')\n",
    "\n",
    "        finally:\n",
    "            self.url_visited_ls.append(url)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    #     if counter == 10:\n",
    "    #     self.update_exist_ls()\n",
    "\n",
    "    #     if counter % num_upload == 0:\n",
    "    #     offset = num_upload * upload_counter\n",
    "    #     df = pd.DataFrame(self.article_ls).iloc[offset:]\n",
    "\n",
    "    #     print(f\"rows uploading {offset} - {counter}\")\n",
    "\n",
    "    #     if upload_counter == 0:\n",
    "    #         self.insert_into_sql(df, is_reset=True)\n",
    "    #     else:\n",
    "    #         self.insert_into_sql(df, is_reset=False)\n",
    "\n",
    "    #     upload_counter += 1\n",
    "\n",
    "    # df = pd.DataFrame(self.article_ls).iloc[offset:]\n",
    "\n",
    "    # self.insert_into_sql(df, is_reset=False)\n",
    "\n",
    "    print('RUN: done crawling')\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 02:42:51,076 INFO:Crawling: https://domo-support.domo.com/s/article/360047400753?language=en_US\n",
      "2023-01-25 02:42:51,077 INFO:Crawling: https://domo-support.domo.com/s/topic/0TO5w000000ZaoEGAS/sharing-access-to-cards-and-pages?language=en_US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: done crawling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Crawler>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "url_to_visit_ls = [\n",
    "    'https://domo-support.domo.com/s/article/360047400753?language=en_US', 'https://domo-support.domo.com/s/topic/0TO5w000000ZaoEGAS/sharing-access-to-cards-and-pages?language=en_US']\n",
    "\n",
    "cd = Crawler(sqlite_driver_db='kb_scrape', base_url='', url_to_visit_ls= url_to_visit_ls)\n",
    "\n",
    "cd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(Crawler)\n",
    "def download_url(self: Crawler, url, debug_prn: bool = False):\n",
    "\n",
    "  options = webdriver.ChromeOptions()\n",
    "  options.add_argument(\"-headless\")\n",
    "  options.add_argument(\"-no-sandbox\")\n",
    "  options.add_argument(\"-disable-dev-shm-usage\")\n",
    "\n",
    "  wd = webdriver.Chrome(\"chromedriver\", options=options)\n",
    "\n",
    "  if debug_prn:\n",
    "        print(f'downloading data from {url}')\n",
    "\n",
    "  wd.get(url)\n",
    "  time.sleep(5)  # sleeps for 5 seconds to give page time to load\n",
    "  source = wd.page_source\n",
    "\n",
    "  return source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: Service chromedriver unexpectedly exited. Status code was: 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m options\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m-no-sandbox\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m options\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m-disable-dev-shm-usage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m wd \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39;49mChrome(\u001b[39m\"\u001b[39;49m\u001b[39mchromedriver\u001b[39;49m\u001b[39m\"\u001b[39;49m, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m     12\u001b[0m wd\u001b[39m.\u001b[39mget(url)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py:80\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m service:\n\u001b[1;32m     78\u001b[0m     service \u001b[39m=\u001b[39m Service(executable_path, port, service_args, service_log_path)\n\u001b[0;32m---> 80\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     81\u001b[0m     DesiredCapabilities\u001b[39m.\u001b[39;49mCHROME[\u001b[39m\"\u001b[39;49m\u001b[39mbrowserName\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     82\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mgoog\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     83\u001b[0m     port,\n\u001b[1;32m     84\u001b[0m     options,\n\u001b[1;32m     85\u001b[0m     service_args,\n\u001b[1;32m     86\u001b[0m     desired_capabilities,\n\u001b[1;32m     87\u001b[0m     service_log_path,\n\u001b[1;32m     88\u001b[0m     service,\n\u001b[1;32m     89\u001b[0m     keep_alive,\n\u001b[1;32m     90\u001b[0m )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py:101\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mservice cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice \u001b[39m=\u001b[39m service\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mservice\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    105\u001b[0m         command_executor\u001b[39m=\u001b[39mChromiumRemoteConnection(\n\u001b[1;32m    106\u001b[0m             remote_server_addr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice\u001b[39m.\u001b[39mservice_url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m         options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    113\u001b[0m     )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/selenium/webdriver/common/service.py:104\u001b[0m, in \u001b[0;36mService.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_process_still_running()\n\u001b[1;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connectable():\n\u001b[1;32m    106\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/selenium/webdriver/common/service.py:117\u001b[0m, in \u001b[0;36mService.assert_process_still_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m return_code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess\u001b[39m.\u001b[39mpoll()\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m return_code:\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m WebDriverException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mService \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath\u001b[39m}\u001b[39;00m\u001b[39m unexpectedly exited. Status code was: \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: Service chromedriver unexpectedly exited. Status code was: 1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"-headless\")\n",
    "options.add_argument(\"-no-sandbox\")\n",
    "options.add_argument(\"-disable-dev-shm-usage\")\n",
    "\n",
    "wd = webdriver.Chrome(\"chromedriver\", options=options)\n",
    "\n",
    "wd.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(Crawler)\n",
    "def crawl(self : Crawler, url, debug_prn: bool = False):\n",
    "    \n",
    "    if debug_prn:\n",
    "        print(f\"CRAWL:  starting crawl - {url}\")\n",
    "\n",
    "    html = self.download_url(url, debug_prn=debug_prn)\n",
    "\n",
    "    # article = self.get_article(html, url)\n",
    "\n",
    "    linked_urls_ls = None\n",
    "    # linked_urls_ls = article.get('linked_urls_ls')\n",
    "\n",
    "    if not linked_urls_ls or len(linked_urls_ls) == 0:\n",
    "        return\n",
    "\n",
    "    for url in linked_urls_ls:\n",
    "        self._add_url_to_visit(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_exist_ls(self):\n",
    "#       \"\"\"updates crawler's visited_urls based on my database of existing kbs\"\"\"\n",
    "#       cursor = self.sqlit_con.execute('''SELECT url FROM kb''')\n",
    "\n",
    "#       for row in cursor:\n",
    "#         self.visited_urls.append(row[0])\n",
    "\n",
    "\n",
    "\n",
    "#     def get_linked_urls(self, html, debug_prn: bool = False):\n",
    "#       import urllib.parse as url_parse\n",
    "#       from bs4 import BeautifulSoup\n",
    "\n",
    "#       soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#       linked_urls = []\n",
    "#       for link in soup.find_all('a'):\n",
    "#           path = link.get('href')\n",
    "\n",
    "#           if not path:\n",
    "#             continue\n",
    "\n",
    "#           if path.startswith('/'):\n",
    "#               path = url_parse.urljoin(self.base_url, path)\n",
    "\n",
    "#           if path.startswith(self.base_url):\n",
    "#             linked_urls.append(path)\n",
    "\n",
    "#       return linked_urls\n",
    "\n",
    "\n",
    "\n",
    "#     def insert_into_sql(self, article_df, is_reset: bool = False):\n",
    "#       con = sqlite3.connect(\n",
    "#           \"/content/drive/MyDrive/Colab Notebooks/webscrape_domo/my_db2.db\")\n",
    "\n",
    "#       print(article_df.__dict__)\n",
    "\n",
    "#       article_df.linked_urls = \", \".join(article_df.linked_urls)\n",
    "\n",
    "#       # display(HTML(article_df.to_html()))\n",
    "\n",
    "#       if is_reset:\n",
    "#         article_df.to_sql(\"kb\", con, if_exists=\"replace\")\n",
    "\n",
    "#       article_df.to_sql(\"kb\", con, if_exists=\"append\")\n",
    "\n",
    "#     def get_article(self, html, url):\n",
    "#       soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#       form_labels = soup.find_all(class_=['slds-form-element'])\n",
    "\n",
    "#       linked_urls_ls = self.get_linked_urls(html)\n",
    "\n",
    "#       article = {\"url\": url,\n",
    "#                  \"linked_urls\": linked_urls_ls}\n",
    "\n",
    "#       for form in form_labels:\n",
    "#         rows = list(form.strings)\n",
    "\n",
    "#         if len(rows) >= 2:\n",
    "#           title = rows[0]\n",
    "#           value_ls = rows[1:]\n",
    "\n",
    "#           value_tx = \" \".join(value_ls)\n",
    "\n",
    "#           value_clean = re.sub(r'( \\n.?)+', r'\\r', value_tx)\n",
    "#           article.update({title: value_clean})\n",
    "\n",
    "#       if article.get('Article Body'):\n",
    "#         self.article_ls.append(article)\n",
    "#       else:\n",
    "#         self.add_url_to_visit(url)\n",
    "\n",
    "#       return article\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linux-5.4.0-1100-azure-x86_64-with-glibc2.31'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.platform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper(object):\n",
    "    def __init__(self, urls):\n",
    "        self.urls = urls\n",
    "        # Global Place To Store The Data:\n",
    "        self.all_data = []\n",
    "        self.master_dict = {}\n",
    "        # Run The Scraper:\n",
    "        asyncio.run(self.main())\n",
    "\n",
    "    async def fetch(self, session, url):\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                # 1. Extracting the Text:\n",
    "                text = await response.text()\n",
    "                # 2. Extracting the  Tag:\n",
    "                title_tag = await self.extract_title_tag(text)\n",
    "                return text, url, title_tag\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "    async def extract_title_tag(self, text):\n",
    "        try:\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            return soup.title\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "    async def main(self):\n",
    "        tasks = []\n",
    "        headers = {\n",
    "            \"user-agent\": \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"}\n",
    "        async with aiohttp.ClientSession(headers=headers) as session:\n",
    "            for url in self.urls:\n",
    "                tasks.append(self.fetch(session, url))\n",
    "\n",
    "            htmls = await asyncio.gather(*tasks)\n",
    "            self.all_data.extend(htmls)\n",
    "\n",
    "            # Storing the raw HTML data.\n",
    "            for html in htmls:\n",
    "                if html is not None:\n",
    "                    url = html[1]\n",
    "                    self.master_dict[url] = {\n",
    "                        'Raw Html': html[0], 'Title': html[2]}\n",
    "                else:\n",
    "                    continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.10/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.10/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1271 sha256=c6b9a562b9a98a1b33b4e3d2ce11b1f25adfee909086f8b83da864a7d5cee901\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/e4/62/1d/d4d1bc4f33350ff84227f89b258edb552d604138e3739f5c83\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyppeteer\n",
      "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.42.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets<11.0,>=10.0\n",
      "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyee<9.0.0,>=8.1.0\n",
      "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Collecting appdirs<2.0.0,>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /home/codespace/.local/lib/python3.10/site-packages (from pyppeteer) (1.26.13)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from pyppeteer) (6.0.0)\n",
      "Requirement already satisfied: certifi>=2021 in /home/codespace/.local/lib/python3.10/site-packages (from pyppeteer) (2022.12.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from importlib-metadata>=1.4->pyppeteer) (3.11.0)\n",
      "Installing collected packages: pyee, appdirs, websockets, tqdm, pyppeteer\n",
      "Successfully installed appdirs-1.4.4 pyee-8.2.2 pyppeteer-1.0.2 tqdm-4.64.1 websockets-10.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyppeteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://domohelp.domo.com/hc/en-us/articles/360043192634-Configuring-a-DataSync-Folder-in-Workbench-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching to root user to install dependencies...\n",
      "++ arch\n",
      "+ [[ x86_64 == \\a\\a\\r\\c\\h\\6\\4 ]]\n",
      "+ [[ ! -f /etc/os-release ]]\n",
      "++ bash -c 'source /etc/os-release && echo $ID'\n",
      "+ ID=ubuntu\n",
      "+ [[ ubuntu != \\u\\b\\u\\n\\t\\u ]]\n",
      "+ dpkg --get-selections\n",
      "+ grep -q '^google-chrome[[:space:]]*install$'\n",
      "+ apt-get update\n",
      "Get:1 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal InRelease [3065 B]\n",
      "Get:2 https://dl.yarnpkg.com/debian stable InRelease [17.1 kB]                 \n",
      "Get:4 https://repo.anaconda.com/pkgs/misc/debrepo/conda stable InRelease [3171 B]\n",
      "Get:5 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal/main amd64 Packages [171 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]                \n",
      "Get:3 https://packagecloud.io/github/git-lfs/ubuntu focal InRelease [24.4 kB]  \n",
      "Get:7 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal/main all Packages [2176 B]\n",
      "Err:2 https://dl.yarnpkg.com/debian stable InRelease                           \n",
      "  The following signatures were invalid: EXPKEYSIG 23E7166788B63E1E Yarn Packaging <yarn@dan.cx>\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:9 https://repo.anaconda.com/pkgs/misc/debrepo/conda stable/main amd64 Packages [1970 B]\n",
      "Get:10 https://packagecloud.io/github/git-lfs/ubuntu focal/main amd64 Packages [3134 B]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]       \n",
      "Get:12 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2448 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]    \n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1882 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.7 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [991 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2009 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1291 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2921 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Reading package lists... Done                           \n",
      "W: GPG error: https://dl.yarnpkg.com/debian stable InRelease: The following signatures were invalid: EXPKEYSIG 23E7166788B63E1E Yarn Packaging <yarn@dan.cx>\n",
      "E: The repository 'https://dl.yarnpkg.com/debian stable InRelease' is not signed.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "Failed to install browsers\n",
      "Error: Failed to install chrome\n"
     ]
    }
   ],
   "source": [
    "# install playwright package:\n",
    "# !pip install playwright\n",
    "# install playwright chrome and firefox browsers\n",
    "!playwright install chrome firefox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mplaywright\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msync_api\u001b[39;00m \u001b[39mimport\u001b[39;00m sync_playwright\n\u001b[0;32m----> 3\u001b[0m \u001b[39mwith\u001b[39;00m sync_playwright() \u001b[39mas\u001b[39;00m pw:\n\u001b[1;32m      4\u001b[0m     browser \u001b[39m=\u001b[39m pw\u001b[39m.\u001b[39mchromium\u001b[39m.\u001b[39mlaunch(headless\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     context \u001b[39m=\u001b[39m browser\u001b[39m.\u001b[39mnew_context(viewport\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1920\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1080\u001b[39m})\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/playwright/sync_api/_context_manager.py:44\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_own_loop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop\u001b[39m.\u001b[39mis_running():\n\u001b[0;32m---> 44\u001b[0m             \u001b[39mraise\u001b[39;00m Error(\n\u001b[1;32m     45\u001b[0m                 \u001b[39m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m             )\n\u001b[1;32m     49\u001b[0m         \u001b[39m# In Python 3.7, asyncio.Process.wait() hangs because it does not use ThreadedChildWatcher\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         \u001b[39m# which is used in Python 3.8+. This is unix specific and also takes care about\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         \u001b[39m# cleaning up zombie processes. See https://bugs.python.org/issue35621\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m             sys\u001b[39m.\u001b[39mversion_info[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     54\u001b[0m             \u001b[39mand\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m7\u001b[39m\n\u001b[1;32m     55\u001b[0m             \u001b[39mand\u001b[39;00m sys\u001b[39m.\u001b[39mplatform \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwin32\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(asyncio\u001b[39m.\u001b[39mget_child_watcher(), asyncio\u001b[39m.\u001b[39mSafeChildWatcher)\n\u001b[1;32m     57\u001b[0m         ):\n",
      "\u001b[0;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "with sync_playwright() as pw:\n",
    "    browser = pw.chromium.launch(headless=False)\n",
    "    context = browser.new_context(viewport={\"width\": 1920, \"height\": 1080})\n",
    "    page = context.new_page()\n",
    "\n",
    "    page.goto(\"https://twitch.tv/directory/game/Art\")  # go to url\n",
    "    # wait for content to load\n",
    "    page.wait_for_selector(\"div[data-target=directory-first-item]\")\n",
    "\n",
    "    # parsed = []\n",
    "    # stream_boxes = page.locator(\n",
    "    #     \"//div[contains(@class,'tw-tower')]/div[@data-target]\")\n",
    "    # for box in stream_boxes.element_handles():\n",
    "    #     parsed.append({\n",
    "    #         \"title\": box.query_selector(\"h3\").inner_text(),\n",
    "    #         \"url\": box.query_selector(\".tw-link\").get_attribute(\"href\"),\n",
    "    #         \"username\": box.query_selector(\".tw-link\").inner_text(),\n",
    "    #         \"viewers\": box.query_selector(\".tw-media-card-stat\").inner_text(),\n",
    "    #         # tags are not always present:\n",
    "    #         \"tags\": box.query_selector(\".tw-tag\").inner_text() if box.query_selector(\".tw-tag\") else None,\n",
    "    #     })\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests_html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Collecting w3lib\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting parse\n",
      "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from requests_html) (2.28.1)\n",
      "Collecting pyquery\n",
      "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: bs4 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from requests_html) (0.0.1)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from requests_html) (1.0.2)\n",
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-1.1.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests_html) (6.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests_html) (4.64.1)\n",
      "Collecting pyee<9.0.0,>=8.1.0\n",
      "  Using cached pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /home/codespace/.local/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests_html) (1.26.13)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests_html) (10.4)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2021 in /home/codespace/.local/lib/python3.10/site-packages (from pyppeteer>=0.0.14->requests_html) (2022.12.7)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.10/site-packages (from bs4->requests_html) (4.11.1)\n",
      "Collecting cssselect>=1.2.0\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: lxml>=2.1 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from pyquery->requests_html) (4.9.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->requests_html) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->requests_html) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.11.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.10/site-packages (from beautifulsoup4->bs4->requests_html) (2.3.2.post1)\n",
      "Building wheels for collected packages: parse\n",
      "  Building wheel for parse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=51c4cb2d5e04eb92dbddae7ef79f589092f9e699316b37404d6698dba6e68d99\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/03/d9/92/db136347b5bcba7d271a3c042ce8c9c279e0ecd79173bb0a6e\n",
      "Successfully built parse\n",
      "Installing collected packages: pyee, parse, fake-useragent, w3lib, cssselect, pyquery, requests_html\n",
      "  Attempting uninstall: pyee\n",
      "    Found existing installation: pyee 9.0.4\n",
      "    Uninstalling pyee-9.0.4:\n",
      "      Successfully uninstalled pyee-9.0.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "playwright 1.30.0 requires pyee==9.0.4, but you have pyee 8.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cssselect-1.2.0 fake-useragent-1.1.1 parse-1.19.0 pyee-8.2.2 pyquery-2.0.0 requests_html-0.10.0 w3lib-2.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m session \u001b[39m=\u001b[39m AsyncHTMLSession()\n\u001b[1;32m      7\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m session\u001b[39m.\u001b[39mget(url)\n\u001b[0;32m----> 9\u001b[0m \u001b[39mawait\u001b[39;00m results\u001b[39m.\u001b[39mhtml\u001b[39m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/requests_html.py:598\u001b[0m, in \u001b[0;36mHTML.render\u001b[0;34m(self, retries, script, wait, scrolldown, sleep, reload, timeout, keep_page)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m content:\n\u001b[1;32m    596\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 598\u001b[0m         content, result, page \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mloop\u001b[39m.\u001b[39;49mrun_until_complete(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_async_render(url\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, script\u001b[39m=\u001b[39;49mscript, sleep\u001b[39m=\u001b[39;49msleep, wait\u001b[39m=\u001b[39;49mwait, content\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhtml, reload\u001b[39m=\u001b[39;49mreload, scrolldown\u001b[39m=\u001b[39;49mscrolldown, timeout\u001b[39m=\u001b[39;49mtimeout, keep_page\u001b[39m=\u001b[39;49mkeep_page))\n\u001b[1;32m    599\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/asyncio/base_events.py:622\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \n\u001b[1;32m    613\u001b[0m \u001b[39mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[39mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[0;32m--> 622\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_running()\n\u001b[1;32m    624\u001b[0m new_task \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m futures\u001b[39m.\u001b[39misfuture(future)\n\u001b[1;32m    625\u001b[0m future \u001b[39m=\u001b[39m tasks\u001b[39m.\u001b[39mensure_future(future, loop\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/asyncio/base_events.py:582\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_running\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    581\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_running():\n\u001b[0;32m--> 582\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThis event loop is already running\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    583\u001b[0m     \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    585\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mCannot run the event loop while another loop is running\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "url = 'https://domohelp.domo.com/hc/en-us/articles/360047400753-jupyter-workspaces#11.0.5.'\n",
    "\n",
    "\n",
    "session = AsyncHTMLSession()\n",
    "results = await session.get(url)\n",
    "\n",
    "await results.html.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
