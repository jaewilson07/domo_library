{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> a class based approach for interacting with Domo Datasets\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: dataset_class.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp classes.DomoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import json\n",
    "\n",
    "import aiohttp\n",
    "\n",
    "# import datetime as dt\n",
    "\n",
    "# import asyncio\n",
    "# import importlib\n",
    "# import io\n",
    "# import json\n",
    "# from enum import Enum, auto\n",
    "# from pprint import pprint\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# from ..utils import Exceptions as ex\n",
    "# from ..utils.Base import Base\n",
    "# from ..utils.chunk_execution import chunk_list\n",
    "# from . import DomoCertification as dmdc\n",
    "# from . import DomoPDP as dmpdp\n",
    "# from . import DomoTag as dmtg\n",
    "\n",
    "\n",
    "import domolibrary.utils.DictDot as util_dd\n",
    "import domolibrary.client.DomoAuth as dmda\n",
    "import domolibrary.routes.dataset as dataset_routes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Classes\n",
    "\n",
    "The `DomoDataset_Schema` class will be a subclass of `DomoDataset`. It will handle all of the methods for interacting with schemas.\n",
    "\n",
    "- In execution, the schema is separate from the data that gets uploaded from Vault to Adrenaline. The domo schema defines how the data is loaded into Vault.\n",
    "- Be cognizant to match dataset uploads with schema definitions. If the schema and uploaded data types do not match, the dataset may be unable to index in Adrenaline (and therefore not update).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DatasetSchema_AuthNotProvidedError(Exception):\n",
    "    \"\"\"return if DatasetSchema request cannot access an auth object\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_id):\n",
    "        message = f\"valid Auth object not provided to dataset - {dataset_id}\"\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "class DatasetSchema_DatasetNotProvidedError(Exception):\n",
    "    \"\"\"return if DatasetSchema request does not have a dataset id\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        message = f\"dataset_id not provided\"\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DomoDataset_Schema_Column:\n",
    "    name: str\n",
    "    id: str\n",
    "    type: str\n",
    "\n",
    "    @classmethod\n",
    "    def _from_json(cls, json_obj):\n",
    "        dd = util_dd.DictDot(json_obj)\n",
    "        return cls(name=dd.name, id=dd.id, type=dd.type)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DomoDataset_Schema:\n",
    "    \"\"\"class for interacting with dataset schemas\"\"\"\n",
    "\n",
    "    dataset: any = None\n",
    "    columns: List[DomoDataset_Schema_Column] = field(default_factory=list)\n",
    "\n",
    "    async def get(\n",
    "        self,\n",
    "        auth: Optional[dmda.DomoAuth] = None,\n",
    "        dataset_id: str = None,\n",
    "        debug_api: bool = False,\n",
    "        return_raw_res: bool = False,  # return the raw response\n",
    "    ) -> List[DomoDataset_Schema_Column]:\n",
    "\n",
    "        \"\"\"method that retrieves schema for a dataset\"\"\"\n",
    "\n",
    "        if self.dataset and (not self.dataset.auth and not auth):\n",
    "            raise DatasetSchema_AuthNotProvidedError(dataset_id = self.dataset.id)\n",
    "\n",
    "        auth = auth or self.dataset.auth\n",
    "\n",
    "        if not self.dataset and not dataset_id:\n",
    "            raise DatasetSchema_DatasetNotProvidedError()\n",
    "\n",
    "        dataset_id = dataset_id or self.dataset.id\n",
    "\n",
    "        res = await dataset_routes.get_schema(\n",
    "            auth=auth, dataset_id=dataset_id, debug_api=debug_api\n",
    "        )\n",
    "\n",
    "        if return_raw_res:\n",
    "            return res.response\n",
    "\n",
    "        if res.status == 200:\n",
    "            json_list = res.response.get(\"tables\")[0].get(\"columns\")\n",
    "\n",
    "            self.columns = [\n",
    "                DomoDataset_Schema_Column._from_json(json_obj=json_obj)\n",
    "                for json_obj in json_list\n",
    "            ]\n",
    "\n",
    "            return self.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/jaewilson07/domo_library/blob/main/domolibrary/classes/DomoDataset.py#L78){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DomoDataset_Schema.get\n",
       "\n",
       ">      DomoDataset_Schema.get\n",
       ">                              (auth:Optional[domolibrary.client.DomoAuth.DomoAu\n",
       ">                              th]=None, dataset_id:str=None,\n",
       ">                              debug_api:bool=False, return_raw_res:bool=False)\n",
       "\n",
       "method that retrieves schema for a dataset"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/jaewilson07/domo_library/blob/main/domolibrary/classes/DomoDataset.py#L78){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DomoDataset_Schema.get\n",
       "\n",
       ">      DomoDataset_Schema.get\n",
       ">                              (auth:Optional[domolibrary.client.DomoAuth.DomoAu\n",
       ">                              th]=None, dataset_id:str=None,\n",
       ">                              debug_api:bool=False, return_raw_res:bool=False)\n",
       "\n",
       "method that retrieves schema for a dataset"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DomoDataset_Schema.get)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample implementation of getting a dataset schema\n",
    "\n",
    "Standard implementation will be to access the `DomoDataset_Schema` class as the `DomoDataset.schema` property\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DomoDataset_Schema_Column(name='objectID', id='objectID', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='url', id='url', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='Title', id='Title', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='article', id='article', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='views', id='views', type='LONG'),\n",
       " DomoDataset_Schema_Column(name='created_dt', id='created_dt', type='DATETIME'),\n",
       " DomoDataset_Schema_Column(name='published_dt', id='published_dt', type='DATETIME')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_schema = DomoDataset_Schema()\n",
    "\n",
    "await ds_schema.get(auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class DatasetTags_AuthNotProvidedError(Exception):\n",
    "    \"\"\"return if DatasetTags request cannot access an auth object\"\"\"\n",
    "\n",
    "    def __init__(self, id):\n",
    "        message = f\"valid Auth object not provided to dataset - {id}\"\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "class DatasetTags_SetTagsError(Exception):\n",
    "    \"\"\"return if DatasetTags request is not successfull\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_id, domo_instance):\n",
    "        message = f\"failed to set tags on dataset - {dataset_id} in {domo_instance}\"\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DomoDataset_Tags:\n",
    "    \"\"\"class for interacting with dataset tags\"\"\"\n",
    "\n",
    "    dataset: any = None\n",
    "    tag_ls: List[str] = field(default_factory=list)\n",
    "\n",
    "    def _have_prereqs(self, auth, dataset_id):\n",
    "        \"\"\"tests if have a parent dataset or prerequsite dataset_id and auth object\"\"\"\n",
    "\n",
    "        if self.dataset and (not self.dataset.auth and not auth):\n",
    "            raise DatasetTags_AuthNotProvidedError(self.dataset.id)\n",
    "\n",
    "        auth = auth or self.dataset.auth\n",
    "\n",
    "        if not self.dataset and not auth:\n",
    "            raise DatasetTags_AuthNotProvidedError(self.dataset.id)\n",
    "\n",
    "        dataset_id = dataset_id or self.dataset.id\n",
    "\n",
    "        return auth, dataset_id\n",
    "\n",
    "    async def get(\n",
    "        self,\n",
    "        dataset_id: str = None,\n",
    "        auth: Optional[dmda.DomoAuth] = None,\n",
    "        debug_api: bool = False,\n",
    "        session: Optional[aiohttp.ClientSession] = None,\n",
    "    ) -> List[str]:  # returns a list of tags\n",
    "        \"\"\"gets the existing list of dataset_tags\"\"\"\n",
    "\n",
    "        auth, dataset_id = self._have_prereqs(auth=auth, dataset_id=dataset_id)\n",
    "\n",
    "        res = await dataset_routes.get_dataset_by_id(\n",
    "            dataset_id=dataset_id, auth=auth, debug_api=debug_api, session=session\n",
    "        )\n",
    "\n",
    "        if res.is_success == False:\n",
    "            print(res)\n",
    "            return None\n",
    "\n",
    "        if res.is_success == True:\n",
    "            tag_ls = json.loads(res.response.get(\"tags\"))\n",
    "            self.tag_ls = tag_ls\n",
    "\n",
    "            return tag_ls\n",
    "\n",
    "    async def set(\n",
    "        self,\n",
    "        tag_ls: [str],\n",
    "        dataset_id: str = None,\n",
    "        auth: Optional[dmda.DomoAuth] = None,\n",
    "        debug_api: bool = False,\n",
    "        session: Optional[aiohttp.ClientSession] = None,\n",
    "    ) -> List[str]: # returns a list of tags\n",
    "        \"\"\"replaces all tags with a new list of dataset_tags\"\"\"\n",
    "\n",
    "        auth, dataset_id = self._have_prereqs(auth=auth, dataset_id=dataset_id)\n",
    "\n",
    "        res = await dataset_routes.set_dataset_tags(\n",
    "            auth=auth,\n",
    "            tag_ls=list(set(tag_ls)),\n",
    "            dataset_id=dataset_id,\n",
    "            debug_api=debug_api,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "        if res.status != 200:\n",
    "            raise DatasetTags_SetTagsError(\n",
    "                dataset_id=dataset_id, domo_instance=auth.domo_instance\n",
    "            )\n",
    "\n",
    "        await self.get(dataset_id=dataset_id, auth=auth)\n",
    "\n",
    "        return self.tag_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['developer_documentation', 'Jan-11-2023 12:50', 'hackercore']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_tag = DomoDataset_Tags()\n",
    "await ds_tag.get(auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['developer_documentation', 'hackercore', 'Jan-11-2023 12:50']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "today = dt.datetime.now().strftime(\"%b-%d-%Y %H:%M\")\n",
    "ds_tag = DomoDataset_Tags()\n",
    "await ds_tag.set(\n",
    "    auth=token_auth,\n",
    "    dataset_id=os.environ[\"DOJO_DATASET_ID\"],\n",
    "    tag_ls=[\"developer_documentation\", \"hackercore\", today],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "async def add(\n",
    "    self: DomoDataset_Tags,\n",
    "    add_tag_ls: [str],\n",
    "    dataset_id: str = None,\n",
    "    auth: Optional[dmda.DomoAuth] = None,\n",
    "    debug_api: bool = False,\n",
    "    session: Optional[aiohttp.ClientSession] = None,\n",
    ") -> List[str]:  # returns a list of tags\n",
    "    \"\"\"appends tags to the list of existing dataset_tags\"\"\"\n",
    "\n",
    "    auth, dataset_id = self._have_prereqs(auth=auth, dataset_id=dataset_id)\n",
    "\n",
    "    existing_tag_ls = await self.get(dataset_id=dataset_id, auth=auth)\n",
    "    add_tag_ls += existing_tag_ls\n",
    "\n",
    "    return await self.set(\n",
    "        auth=auth,\n",
    "        dataset_id=dataset_id,\n",
    "        tag_ls=list(set(add_tag_ls)),\n",
    "        debug_api=debug_api,\n",
    "        session=session,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023', 'developer_documentation', 'hackercore', 'Jan-11-2023 12:50']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "today_year = dt.datetime.today().strftime(\"%Y\")\n",
    "ds_tag = DomoDataset_Tags()\n",
    "await ds_tag.add(\n",
    "    auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"], add_tag_ls=[today_year]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "async def remove(self: DomoDataset_Tags,\n",
    "                 remove_tag_ls: [str],\n",
    "                 dataset_id: str = None,\n",
    "                 auth: dmda.DomoFullAuth = None,\n",
    "                 debug_api: bool = False,\n",
    "                 session: Optional[aiohttp.ClientSession] = None\n",
    "                 ) -> List[str]:  # returns a list of tags\n",
    "    \"\"\"removes tags from the existing list of dataset_tags\"\"\"\n",
    "\n",
    "    auth, dataset_id = self._have_prereqs(auth=auth, dataset_id=dataset_id)\n",
    "\n",
    "    existing_tag_ls = await self.get(dataset_id=dataset_id, auth=auth)\n",
    "\n",
    "    existing_tag_ls = [\n",
    "        ex for ex in existing_tag_ls if ex not in remove_tag_ls]\n",
    "\n",
    "    return await self.set(auth=auth,\n",
    "                          dataset_id=dataset_id,\n",
    "                          tag_ls=list(set(existing_tag_ls)),\n",
    "                          debug_api=debug_api, session=session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['developer_documentation', 'hackercore', 'Jan-11-2023 12:50']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "today_year = dt.datetime.today().strftime(\"%Y\")\n",
    "\n",
    "ds_tag = DomoDataset_Tags()\n",
    "\n",
    "await ds_tag.remove(\n",
    "    auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"], remove_tag_ls=[ today_year])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN - Domo Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class DomoDataset:\n",
    "    \"interacts with domo datasets\"\n",
    "\n",
    "    auth: dmda.DomoAuth = field(repr=False, default=None)\n",
    "\n",
    "    id: str = \"\"\n",
    "    display_type: str = \"\"\n",
    "    data_provider_type: str = \"\"\n",
    "    name: str = \"\"\n",
    "    description: str = \"\"\n",
    "    row_count: int = None\n",
    "    column_count: int = None\n",
    "\n",
    "    stream_id: int = None\n",
    "\n",
    "    owner: dict = field(default_factory=dict)\n",
    "    formula: dict = field(default_factory=dict)\n",
    "\n",
    "    schema: DomoDataset_Schema = field(default=None)\n",
    "    # tags: Dataset_Tags = field(default = None)\n",
    "\n",
    "    # certification: dmdc.DomoCertification = None\n",
    "    # PDPPolicies: dmpdp.Dataset_PDP_Policies = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.schema = DomoDataset_Schema(dataset=self)\n",
    "        # self.tags = Dataset_Tags(dataset=self)\n",
    "\n",
    "        # self.PDPPolicies = dmpdp.Dataset_PDP_Policies(self)\n",
    "\n",
    "    def display_url(self):\n",
    "        return f\"https://{self.auth.domo_instance }.domo.com/datasources/{self.id}/details/overview\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample class-based implementation of get schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>visible</th>\n",
       "      <th>order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>objectID</td>\n",
       "      <td>objectID</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>url</td>\n",
       "      <td>url</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title</td>\n",
       "      <td>Title</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article</td>\n",
       "      <td>article</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>views</td>\n",
       "      <td>views</td>\n",
       "      <td>LONG</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>created_dt</td>\n",
       "      <td>created_dt</td>\n",
       "      <td>DATETIME</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>published_dt</td>\n",
       "      <td>published_dt</td>\n",
       "      <td>DATETIME</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name            id      type  visible  order\n",
       "0      objectID      objectID    STRING     True      0\n",
       "1           url           url    STRING     True      0\n",
       "2         Title         Title    STRING     True      0\n",
       "3       article       article    STRING     True      0\n",
       "4         views         views      LONG     True      0\n",
       "5    created_dt    created_dt  DATETIME     True      0\n",
       "6  published_dt  published_dt  DATETIME     True      0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this sample returns raw response from the api\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds = DomoDataset(auth=token_auth, id=os.environ[\"DOJO_DATASET_ID\"])\n",
    "\n",
    "raw_res = await ds.schema.get(return_raw_res=True)\n",
    "\n",
    "pd.DataFrame(raw_res.get(\"tables\")[0].get(\"columns\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DomoDataset_Schema_Column(name='objectID', id='objectID', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='url', id='url', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='Title', id='Title', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='article', id='article', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='views', id='views', type='LONG'),\n",
       " DomoDataset_Schema_Column(name='created_dt', id='created_dt', type='DATETIME'),\n",
       " DomoDataset_Schema_Column(name='published_dt', id='published_dt', type='DATETIME')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this sample returns class-based response from the api\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds = DomoDataset(auth=token_auth, id=os.environ[\"DOJO_DATASET_ID\"])\n",
    "\n",
    "await ds.schema.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "async def get_from_id(\n",
    "    cls: DomoDataset,\n",
    "    dataset_id: str,\n",
    "    auth: dmda.DomoAuth,\n",
    "    debug_api: bool = False,\n",
    "    return_raw_res: bool = False,\n",
    "):\n",
    "\n",
    "    \"\"\"retrieves dataset metadata\"\"\"\n",
    "\n",
    "    res = await dataset_routes.get_dataset_by_id(\n",
    "        auth=auth, dataset_id=dataset_id, debug_api=debug_api\n",
    "    )\n",
    "\n",
    "    if return_raw_res:\n",
    "        return res.response\n",
    "\n",
    "    dd = util_dd.DictDot(res.response)\n",
    "    ds = cls(\n",
    "        auth=auth,\n",
    "        id=dd.id,\n",
    "        display_type=dd.displayType,\n",
    "        data_provider_type=dd.dataProviderType,\n",
    "        name=dd.name,\n",
    "        description=dd.description,\n",
    "        owner=dd.owner,\n",
    "        formula=dd.properties.formulas.formulas,\n",
    "        stream_id=dd.streamId,\n",
    "        row_count=int(dd.rowCount),\n",
    "        column_count=int(dd.columnCount),\n",
    "    )\n",
    "\n",
    "    # if dd.tags:\n",
    "    #     ds.tags.tag_ls = json.loads(dd.tags)\n",
    "\n",
    "    # if dd.certification:\n",
    "    #     # print('class def certification', dd.certification)\n",
    "    #     ds.certification = dmdc.DomoCertification._from_json(\n",
    "    #         dd.certification)\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample implementation of get_from_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset - 123 not found in domo-dojo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "try:\n",
    "    await DomoDataset.get_from_id(auth=token_auth, dataset_id=\"123\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DomoDataset(id='04c1574e-c8be-4721-9846-c6ffa491144b', display_type='domo-jupyterdata', data_provider_type='domo-jupyterdata', name='domo_kbs', description=None, row_count=1185, column_count=7, stream_id=825, owner=DictDot(id='1893952720', name='Jae Wilson', type='USER', group=False), formula=DictDot(), schema=DomoDataset_Schema(dataset=..., columns=[]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-dojo\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "await DomoDataset.get_from_id(auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     @classmethod\n",
    "#     async def query_dataset(cls,\n",
    "#                             sql: str,\n",
    "#                             dataset_id: str,\n",
    "#                             dev_auth: DomoDeveloperAuth,\n",
    "#                             debug: bool = False,\n",
    "#                             session: aiohttp.ClientSession = None) -> pd.DataFrame:\n",
    "\n",
    "#         if debug:\n",
    "#             print(\"query dataset class method\")\n",
    "#             print({'dataset_id': dataset_id,\n",
    "#                    'dev_auth': dev_auth})\n",
    "\n",
    "#         res = await dataset_routes.query_dataset_public(dev_auth=dev_auth, id=dataset_id, sql=sql, session=session,\n",
    "#                                                         debug=debug)\n",
    "\n",
    "#         if debug:\n",
    "#             print(res.response)\n",
    "\n",
    "#         if res.status == 200:\n",
    "#             df = pd.DataFrame(data=res.response.get('rows'),\n",
    "#                               columns=res.response.get('columns'))\n",
    "#             return df\n",
    "#         return None\n",
    "\n",
    "#     @classmethod\n",
    "#     async def query_dataset_private(cls,\n",
    "#                                     sql: str,\n",
    "#                                     dataset_id: str,\n",
    "#                                     full_auth: DomoFullAuth,\n",
    "#                                     debug: bool = False,\n",
    "#                                     session: aiohttp.ClientSession = None) -> pd.DataFrame:\n",
    "\n",
    "#         if debug:\n",
    "#             print(\"query dataset class method\")\n",
    "#             print({'dataset_id': dataset_id,\n",
    "#                    'full_auth': full_auth})\n",
    "\n",
    "#         res = await dataset_routes.query_dataset_private(full_auth=full_auth, id=dataset_id, sql=sql, session=session,\n",
    "#                                                          debug=debug)\n",
    "\n",
    "#         return pd.DataFrame(res)\n",
    "\n",
    "#     async def upload_csv(self,\n",
    "#                          upload_df: pd.DataFrame = None,\n",
    "#                          upload_df_list: list[pd.DataFrame] = None,\n",
    "#                          upload_file: io.TextIOWrapper = None,\n",
    "\n",
    "#                          full_auth: DomoFullAuth = None,\n",
    "#                          upload_method: str = 'REPLACE',\n",
    "#                          dataset_id: str = None,\n",
    "#                          dataset_upload_id=None,\n",
    "#                          partition_key: str = None,\n",
    "#                          is_index: bool = True,\n",
    "#                          session: aiohttp.ClientSession = None,\n",
    "#                          debug: bool = False):\n",
    "\n",
    "#         full_auth = full_auth or self.full_auth\n",
    "#         dataset_id = dataset_id or self.id\n",
    "\n",
    "#         upload_df_list = upload_df_list or [upload_df]\n",
    "\n",
    "#         # stage 1 get uploadId\n",
    "#         if not dataset_upload_id:\n",
    "#             if debug:\n",
    "#                 print(f\"\\n\\nðŸŽ­ starting Stage 1\")\n",
    "\n",
    "#             res = await dataset_routes.upload_dataset_stage_1(full_auth=full_auth,\n",
    "#                                                               dataset_id=dataset_id,\n",
    "#                                                               session=session,\n",
    "#                                                               data_tag=partition_key,\n",
    "#                                                               debug=debug\n",
    "#                                                               )\n",
    "#             if debug:\n",
    "#                 print(f\"\\n\\nðŸŽ­ Stage 1 response -- {res.status}\")\n",
    "#                 print(res)\n",
    "\n",
    "#             dataset_upload_id = res.response.get('uploadId')\n",
    "\n",
    "#         # stage 2 upload_dataset\n",
    "\n",
    "#         if debug:\n",
    "#             print(\n",
    "#                 f\"\\n\\nðŸŽ­ starting Stage 2 - {len(upload_df_list)} - number of parts\")\n",
    "\n",
    "#         stage_2_res = None\n",
    "\n",
    "#         if upload_file:\n",
    "#             if debug:\n",
    "#                 print('stage 2 - file')\n",
    "#             stage_2_res = await dataset_routes.upload_dataset_stage_2_file(full_auth=full_auth,\n",
    "#                                                                            dataset_id=dataset_id,\n",
    "#                                                                            upload_id=dataset_upload_id,\n",
    "#                                                                            part_id=1,\n",
    "#                                                                            file=upload_file,\n",
    "#                                                                            session=session, debug=debug)\n",
    "#             if debug:\n",
    "#                 print(f\"ðŸŽ­ Stage 2 response -- {stage_2_res.status}\")\n",
    "#                 print(stage_2_res.print(is_pretty=True))\n",
    "\n",
    "#         else:\n",
    "#             if debug:\n",
    "#                 print('stage 2 - df')\n",
    "#             stage_2_res = await asyncio.gather(*[dataset_routes.upload_dataset_stage_2_df(full_auth=full_auth,\n",
    "#                                                                                           dataset_id=dataset_id,\n",
    "#                                                                                           upload_id=dataset_upload_id,\n",
    "#                                                                                           part_id=index + 1,\n",
    "#                                                                                           upload_df=df,\n",
    "#                                                                                           session=session, debug=debug) for index, df in enumerate(upload_df_list)])\n",
    "\n",
    "#             if debug:\n",
    "#                 for res in stage_2_res:\n",
    "#                     print(f\"ðŸŽ­ Stage 2 response -- {res.status}\")\n",
    "#                     res.print(is_pretty=True)\n",
    "\n",
    "#         # return stage_2_res\n",
    "\n",
    "# #         # stage 3 commit_data\n",
    "#         if debug:\n",
    "#             print(f\"\\n\\nðŸŽ­ starting Stage 3\")\n",
    "#         await asyncio.sleep(10)\n",
    "\n",
    "#         stage3_res = await dataset_routes.upload_dataset_stage_3(full_auth=full_auth,\n",
    "#                                                                  dataset_id=dataset_id,\n",
    "#                                                                  upload_id=dataset_upload_id,\n",
    "#                                                                  update_method=upload_method,\n",
    "#                                                                  data_tag=partition_key,\n",
    "#                                                                  is_index=False,\n",
    "#                                                                  session=session,\n",
    "#                                                                  debug=debug)\n",
    "\n",
    "#         if debug:\n",
    "#             print(f\"\\nðŸŽ­ stage 3 res - {res.status}\")\n",
    "#             print(stage3_res)\n",
    "\n",
    "#         if is_index:\n",
    "#             await self.index_dataset(full_auth=full_auth,\n",
    "#                                      dataset_id=dataset_id,\n",
    "#                                      debug=debug,\n",
    "#                                      session=session)\n",
    "\n",
    "#         return stage3_res\n",
    "\n",
    "#     async def index_dataset(self,\n",
    "#                             full_auth: DomoFullAuth = None,\n",
    "#                             dataset_id: str = None,\n",
    "#                             debug: bool = False,\n",
    "#                             session: aiohttp.ClientSession = None\n",
    "#                             ):\n",
    "\n",
    "#         full_auth = full_auth or self.full_auth\n",
    "#         dataset_id = dataset_id or self.id\n",
    "#         return await dataset_routes.index_dataset(full_auth=full_auth, dataset_id=dataset_id, debug=debug,\n",
    "#                                                   session=session)\n",
    "\n",
    "#     async def list_partitions(self,\n",
    "#                               full_auth: DomoFullAuth = None,\n",
    "#                               dataset_id: str = None,\n",
    "#                               debug: bool = False,\n",
    "#                               session: aiohttp.ClientSession = None\n",
    "#                               ):\n",
    "\n",
    "#         full_auth = full_auth or self.full_auth\n",
    "#         dataset_id = dataset_id or self.id\n",
    "\n",
    "#         res = await dataset_routes.list_partitions(full_auth=full_auth, dataset_id=dataset_id, debug=debug,\n",
    "#                                                    session=session)\n",
    "#         if res.status != 200:\n",
    "#             return None\n",
    "#         return res.response\n",
    "\n",
    "#     async def delete_partition(self,\n",
    "#                                dataset_partition_id: str,\n",
    "\n",
    "#                                dataset_id: str = None,\n",
    "#                                empty_df: pd.DataFrame = None,\n",
    "\n",
    "#                                full_auth: DomoFullAuth = None,\n",
    "\n",
    "#                                is_index: bool = True,\n",
    "#                                debug: bool = False,\n",
    "#                                session: aiohttp.ClientSession = None):\n",
    "\n",
    "#         is_close_session = True if not session else False\n",
    "\n",
    "#         session = session or aiohttp.ClientSession()\n",
    "#         full_auth = full_auth or self.full_auth\n",
    "#         dataset_id = dataset_id or self.id\n",
    "\n",
    "# #        if empty_df is None:\n",
    "# #            empty_df = await self.query_dataset_private(full_auth=full_auth,\n",
    "# #                                                        dataset_id=dataset_id,\n",
    "# #                                                        sql=\"SELECT * from table limit 1\",\n",
    "# #                                                        debug=False)\n",
    "# #\n",
    "# #        await self.upload_csv(upload_df=empty_df.head(0),\n",
    "# #                              upload_method='REPLACE',\n",
    "# #                              is_index=is_index,\n",
    "# #                              partition_key=dataset_partition_id,\n",
    "# #                              session=session,\n",
    "# #                              debug=False)\n",
    "#         if debug:\n",
    "#             print(f\"\\n\\nðŸŽ­ starting Stage 1\")\n",
    "\n",
    "#         res = await dataset_routes.delete_partition_stage_1(full_auth=full_auth,\n",
    "#                                                             dataset_id=dataset_id,\n",
    "#                                                             dataset_partition_id=dataset_partition_id,\n",
    "#                                                             debug=debug, session=session)\n",
    "#         if debug:\n",
    "#             print(f\"\\n\\nðŸŽ­ Stage 1 response -- {res.status}\")\n",
    "#             print(res)\n",
    "\n",
    "#         stage_2_res = None\n",
    "#         if debug:\n",
    "#             print('starting Stage 2')\n",
    "#         stage_2_res = await dataset_routes.delete_partition_stage_2(full_auth=full_auth,\n",
    "#                                                                     dataset_id=dataset_id,\n",
    "#                                                                     dataset_partition_id=dataset_partition_id,\n",
    "#                                                                     debug=debug, session=session)\n",
    "#         if debug:\n",
    "#             print(f\"\\n\\nðŸŽ­ Stage 2 response -- {stage_2_res.status}\")\n",
    "\n",
    "#         stage_3_res = None\n",
    "#         if debug:\n",
    "#             print('starting Stage 3')\n",
    "#         stage_3_res = await dataset_routes.index_dataset(full_auth=full_auth,\n",
    "#                                                          dataset_id=dataset_id,\n",
    "#                                                          debug=debug, session=session)\n",
    "#         if debug:\n",
    "#             print(f\"\\n\\nðŸŽ­ Stage 3 response -- {stage_3_res.status}\")\n",
    "\n",
    "#         if is_close_session:\n",
    "#             await session.close()\n",
    "\n",
    "#         if debug:\n",
    "#             print(stage_3_res)\n",
    "\n",
    "#         if stage_3_res.status == 200:\n",
    "#             return res.response\n",
    "\n",
    "#     async def reset_dataset(self,\n",
    "#                             full_auth: DomoFullAuth = None,\n",
    "#                             is_index: bool = True,\n",
    "#                             debug: bool = False\n",
    "#                             ):\n",
    "#         execute_reset = input(\n",
    "#             \"This function will delete all rows.  Type BLOW_ME_AWAY to execute:\")\n",
    "\n",
    "#         if execute_reset != 'BLOW_ME_AWAY':\n",
    "#             print(\"You didn't type BLOW_ME_AWAY, moving on.\")\n",
    "#             return None\n",
    "\n",
    "#         full_auth = full_auth or self.full_auth\n",
    "#         dataset_id = self.id\n",
    "\n",
    "#         if not full_auth:\n",
    "#             raise Exception(\"full_auth required\")\n",
    "\n",
    "#         session = aiohttp.ClientSession()\n",
    "\n",
    "#         # create empty dataset to retain schema\n",
    "#         empty_df = await self.query_dataset_private(full_auth=full_auth,\n",
    "#                                                     dataset_id=dataset_id,\n",
    "#                                                     sql=\"SELECT * from table limit 1\",\n",
    "#                                                     session=session,\n",
    "#                                                     debug=debug)\n",
    "#         empty_df = empty_df.head(0)\n",
    "\n",
    "#         # get partition list\n",
    "# #         partition_list = await dataset_routes.list_partitions(full_auth=full_auth,\n",
    "# #                                                               dataset_id=self.id,\n",
    "# #                                                               debug=debug,\n",
    "# #                                                               session=session)\n",
    "\n",
    "# #         if len(partition_list) > 0:\n",
    "# #             partition_list = chunk_list(partition_list, 100)\n",
    "\n",
    "# #             for index, pl in enumerate(partition_list):\n",
    "# #                 print(f'ðŸ¥« starting chunk {index + 1} of {len(partition_list)}')\n",
    "\n",
    "# #                 await asyncio.gather(*[self.delete_partition(full_auth=full_auth,\n",
    "# #                                                              dataset_partition_id=partition.get('partitionId'),\n",
    "# #                                                              session=session,\n",
    "# #                                                              empty_df=empty_df,\n",
    "# #                                                              debug=False) for partition in pl])\n",
    "# #                 if is_index:\n",
    "# #                     await self.index_dataset(session=session)\n",
    "\n",
    "#         res = await self.upload_csv(upload_df=empty_df,\n",
    "#                                     upload_method='REPLACE',\n",
    "#                                     is_index=is_index,\n",
    "#                                     session=session,\n",
    "#                                     debug=False)\n",
    "\n",
    "#         await session.close()\n",
    "#         return True\n",
    "\n",
    "#     async def delete(self,\n",
    "#                      dataset_id=None,\n",
    "#                      full_auth: DomoFullAuth = None,\n",
    "#                      debug: bool = False,\n",
    "#                      session: aiohttp.ClientSession = None):\n",
    "#         try:\n",
    "#             is_close_session = False\n",
    "\n",
    "#             if not session:\n",
    "#                 session = aiohttp.ClientSession()\n",
    "#                 is_close_session = True\n",
    "\n",
    "#             return await dataset_routes.delete(\n",
    "#                 full_auth=full_auth or self.full_auth,\n",
    "#                 dataset_id=dataset_id or self.id,\n",
    "#                 debug=debug,\n",
    "#                 session=session)\n",
    "\n",
    "#         finally:\n",
    "#             if is_close_session:\n",
    "#                 await session.close()\n",
    "\n",
    "#     # async def create(self,\n",
    "#     #                   ds_name,\n",
    "#     #                   ds_type ='api',\n",
    "#     #                   schema = { \"columns\": [ {\n",
    "#     #                       \"name\": 'col1',\n",
    "#     #                       \"type\": 'LONG',\n",
    "#     #                       \"metadata\": None,\n",
    "#     #                       \"upsertKey\": False}\n",
    "#     #                   ]},\n",
    "#     #                   full_auth:DomoFullAuth = None,\n",
    "#     #                   debug:bool = False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
