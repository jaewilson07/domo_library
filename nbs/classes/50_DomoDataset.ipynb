{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> a class based approach for interacting with Domo Datasets\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: dataset_class.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | default_exp classes.DomoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# |exporti\n",
    "import json\n",
    "import io\n",
    "\n",
    "import httpx\n",
    "import asyncio\n",
    "\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "from nbdev.showdoc import patch_to\n",
    "\n",
    "import domolibrary.utils.DictDot as util_dd\n",
    "import domolibrary.utils.chunk_execution as ce\n",
    "\n",
    "import domolibrary.client.DomoAuth as dmda\n",
    "import domolibrary.client.DomoError as de\n",
    "\n",
    "import domolibrary.routes.dataset as dataset_routes\n",
    "\n",
    "import domolibrary.classes.DomoPDP as dmpdp\n",
    "import domolibrary.classes.DomoCertification as dmdc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "from domolibrary.routes.dataset import ShareDataset_AccessLevelEnum, DatasetNotFoundError, QueryRequestError, ShareDataset_Error,UploadDataError\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Classes\n",
    "## DatasetSchema\n",
    "\n",
    "The `DomoDataset_Schema` class will be a subclass of `DomoDataset`. It will handle all of the methods for interacting with schemas.\n",
    "\n",
    "- In execution, the schema is separate from the data that gets uploaded from Vault to Adrenaline. The domo schema defines how the data is loaded into Vault.\n",
    "- Be cognizant to match dataset uploads with schema definitions. If the schema and uploaded data types do not match, the dataset may be unable to index in Adrenaline (and therefore not update).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| exporti\n",
    "async def _have_prereqs(self, auth, dataset_id, function_name):\n",
    "    \"\"\"tests if have a parent dataset or prerequsite dataset_id and auth object\"\"\"\n",
    "\n",
    "    auth_from_self_dataset = getattr(self.dataset, 'auth', None) if getattr(self, 'dataset', None) else None\n",
    "    auth_from_self = getattr(self , 'auth', None)\n",
    "\n",
    "    auth = auth or auth_from_self or auth_from_self_dataset\n",
    "\n",
    "    await auth.get_auth_token()\n",
    "\n",
    "    if not auth or not auth.token:\n",
    "        raise de.AuthNotProvidedError(\n",
    "            function_name=function_name,\n",
    "            entity_id = self.dataset.id)\n",
    "\n",
    "    id_from_self = getattr(self, 'id', None)\n",
    "    id_from_self_parent = getattr(self.dataset, 'id', None ) if getattr(self, 'dataset', None) else None\n",
    "    \n",
    "    dataset_id = dataset_id or id_from_self or id_from_self_parent\n",
    "    \n",
    "    if not dataset_id:\n",
    "        raise de.DatasetNotProvidedError(\n",
    "            function_name = function_name, \n",
    "            domo_instance = auth.domo_instance\n",
    "        )\n",
    "\n",
    "    return auth, dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class DatasetSchema_Types(Enum):\n",
    "    STRING = 'STRING'\n",
    "    DOUBLE = 'DOUBLE'\n",
    "    LONG = 'LONG'\n",
    "    DATE = 'DATE'\n",
    "    DATETIME = 'DATETIME'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DomoDataset_Schema_Column:\n",
    "    name: str\n",
    "    id: str\n",
    "    type: DatasetSchema_Types\n",
    "    order: int = 0\n",
    "    visible: bool = True\n",
    "    upsert_key: bool = False\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.id == other.id)\n",
    "        \n",
    "    @classmethod\n",
    "    def _from_json(cls, json_obj):\n",
    "        dd = util_dd.DictDot(json_obj)\n",
    "        return cls(name=dd.name,\n",
    "                   id=dd.id,\n",
    "                   type=dd.type,\n",
    "                   visible = dd.visible or dd.isVisible or True,\n",
    "                   upsert_key = dd.upsertKey or False,\n",
    "                   order = dd.order or 0\n",
    "                   )\n",
    "    \n",
    "    def to_dict(self):\n",
    "        s = self.__dict__\n",
    "        s['upsertKey'] = s.pop('upsert_key') if 'upsert_key' in s else False\n",
    "        return s\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DomoDataset_Schema:\n",
    "    \"\"\"class for interacting with dataset schemas\"\"\"\n",
    "\n",
    "    dataset: any = None\n",
    "    columns: List[DomoDataset_Schema_Column] = field(default_factory=list)\n",
    "\n",
    "    async def get(\n",
    "        self,\n",
    "        auth: Optional[dmda.DomoAuth] = None,\n",
    "        dataset_id: str = None,\n",
    "        debug_api: bool = False,\n",
    "        return_raw: bool = False,  # return the raw response\n",
    "    ) -> List[DomoDataset_Schema_Column]:\n",
    "\n",
    "        \"\"\"method that retrieves schema for a dataset\"\"\"\n",
    "\n",
    "        auth, dataset_id = await _have_prereqs(self=self, auth=auth, dataset_id=dataset_id, function_name=\"DomoDataset_Schema.get\")\n",
    "\n",
    "        res = await dataset_routes.get_schema(\n",
    "            auth=auth, dataset_id=dataset_id, debug_api=debug_api\n",
    "        )\n",
    "\n",
    "        if return_raw:\n",
    "            return res.response\n",
    "\n",
    "        if res.status == 200:\n",
    "            json_list = res.response.get(\"tables\")[0].get(\"columns\")\n",
    "\n",
    "            self.columns = [\n",
    "                DomoDataset_Schema_Column._from_json(json_obj=json_obj)\n",
    "                for json_obj in json_list\n",
    "            ]\n",
    "\n",
    "            return self.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/jaewilson07/domo_library/blob/main/domolibrary/classes/DomoDataset.py#L113){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DomoDataset_Schema.get\n",
       "\n",
       ">      DomoDataset_Schema.get\n",
       ">                              (auth:Optional[domolibrary.client.DomoAuth.DomoAu\n",
       ">                              th]=None, dataset_id:str=None,\n",
       ">                              debug_api:bool=False, return_raw:bool=False)\n",
       "\n",
       "method that retrieves schema for a dataset"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/jaewilson07/domo_library/blob/main/domolibrary/classes/DomoDataset.py#L113){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DomoDataset_Schema.get\n",
       "\n",
       ">      DomoDataset_Schema.get\n",
       ">                              (auth:Optional[domolibrary.client.DomoAuth.DomoAu\n",
       ">                              th]=None, dataset_id:str=None,\n",
       ">                              debug_api:bool=False, return_raw:bool=False)\n",
       "\n",
       "method that retrieves schema for a dataset"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DomoDataset_Schema.get)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample implementation of getting a dataset schema\n",
    "\n",
    "Standard implementation will be to access the `DomoDataset_Schema` class as the `DomoDataset.schema` property\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DomoDataset_Schema_Column(name='objectID', id='objectID', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='url', id='url', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='Title', id='Title', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='article', id='article', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='views', id='views', type='LONG', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='created_dt', id='created_dt', type='DATETIME', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='published_dt', id='published_dt', type='DATETIME', order=0, visible=True, upsert_key=False)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_schema = DomoDataset_Schema()\n",
    "\n",
    "await ds_schema.get(auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "class DatasetSchema_InvalidSchema(de.DomoError):\n",
    "    def __init__(self, domo_instance,\n",
    "                 dataset_id, missing_columns, dataset_name=None):\n",
    "\n",
    "        if dataset_id:\n",
    "            message = f\"{dataset_id}{f' - {dataset_name}' if dataset_name else ''} is missing columns {', '.join(missing_columns)}\"\n",
    "\n",
    "        super().__init__(domo_instance=domo_instance,\n",
    "                       message=message)\n",
    "\n",
    "\n",
    "@patch_to(DomoDataset_Schema)\n",
    "async def _test_missing_columns(self: DomoDataset_Schema,\n",
    "                                df: pd.DataFrame,\n",
    "                                dataset_id=None,\n",
    "                                auth: dmda.DomoAuth = None,\n",
    "                                ):\n",
    "\n",
    "    dataset_id = dataset_id or self.dataset.id\n",
    "    auth = auth or self.dataset.auth\n",
    "\n",
    "    await self.get(dataset_id=dataset_id, auth=auth)\n",
    "\n",
    "    missing_columns = [col for col in df.columns if col not in [\n",
    "        scol.name for scol in self.columns]]\n",
    "\n",
    "    if len(missing_columns) > 0:\n",
    "        raise DatasetSchema_InvalidSchema(domo_instance=auth.domo_instance,\n",
    "                                          dataset_id=dataset_id,\n",
    "                                          missing_columns=missing_columns\n",
    "                                          )\n",
    "        return missing_columns\n",
    "    \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛑  DatasetSchema_InvalidSchema 🛑 - function: 04c1574e-c8be-4721-9846-c6ffa491144b is missing columns A, B, C at domo-community\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import domolibrary.client.DomoAuth as dmda\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_schema = DomoDataset_Schema()\n",
    "\n",
    "test_df =  pd.DataFrame(columns=['A', 'B', 'C' ])\n",
    "\n",
    "try:\n",
    "    res = await ds_schema._test_missing_columns(auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"], df = test_df)\n",
    "    print(res)\n",
    "\n",
    "except DatasetSchema_InvalidSchema as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@ patch_to(DomoDataset_Schema)\n",
    "async def reset_col_order(self: DomoDataset_Schema, df: pd.DataFrame, dataset_id ):\n",
    "\n",
    "    await self.get()\n",
    "\n",
    "\n",
    "    if len(self.columns) != len(df.columns):\n",
    "        raise Exception(\"\")\n",
    "\n",
    "    for index, col in enumerate(self.schema.columns):\n",
    "        col.order=col.order if col.order > 0 else index\n",
    "\n",
    "# for index, schema in enumerate(consol_ds.schema.columns):\n",
    "#     schema.order = index\n",
    "\n",
    "# schema = {'columns': [col.__dict__ for col in consol_ds.schema.columns]}\n",
    "# schema\n",
    "\n",
    "# import domolibrary.routes.dataset as dataset_routes\n",
    "# await dataset_routes.alter_schema(auth = consol_auth, dataset_id = consol_ds.id, schema_obj = schema)\n",
    "\n",
    "    df[[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "@patch_to(DomoDataset_Schema)\n",
    "def to_dict(self: DomoDataset_Schema):\n",
    "    return {\"columns\": [\n",
    "        col.to_dict() for col in self.columns]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columns': [{'name': 'objectID',\n",
       "   'id': 'objectID',\n",
       "   'type': 'STRING',\n",
       "   'order': 0,\n",
       "   'visible': True,\n",
       "   'upsertKey': False},\n",
       "  {'name': 'url',\n",
       "   'id': 'url',\n",
       "   'type': 'STRING',\n",
       "   'order': 0,\n",
       "   'visible': True,\n",
       "   'upsertKey': False},\n",
       "  {'name': 'Title',\n",
       "   'id': 'Title',\n",
       "   'type': 'STRING',\n",
       "   'order': 0,\n",
       "   'visible': True,\n",
       "   'upsertKey': False},\n",
       "  {'name': 'article',\n",
       "   'id': 'article',\n",
       "   'type': 'STRING',\n",
       "   'order': 0,\n",
       "   'visible': True,\n",
       "   'upsertKey': False},\n",
       "  {'name': 'views',\n",
       "   'id': 'views',\n",
       "   'type': 'LONG',\n",
       "   'order': 0,\n",
       "   'visible': True,\n",
       "   'upsertKey': False},\n",
       "  {'name': 'created_dt',\n",
       "   'id': 'created_dt',\n",
       "   'type': 'DATETIME',\n",
       "   'order': 0,\n",
       "   'visible': True,\n",
       "   'upsertKey': False},\n",
       "  {'name': 'published_dt',\n",
       "   'id': 'published_dt',\n",
       "   'type': 'DATETIME',\n",
       "   'order': 0,\n",
       "   'visible': True,\n",
       "   'upsertKey': False}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_schema = DomoDataset_Schema()\n",
    "\n",
    "await ds_schema.get(auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"])\n",
    "\n",
    "ds_schema.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "@patch_to(DomoDataset_Schema)\n",
    "def add_col(self: DomoDataset_Schema, col: DomoDataset_Schema_Column, debug_prn: bool = False):\n",
    "\n",
    "    if col in self.columns and debug_prn:\n",
    "        print(\n",
    "            f\"column - {col.name} already in dataset {self.dataset.name if self.dataset else '' }\")\n",
    "\n",
    "    if col not in self.columns:\n",
    "        self.columns.append(col)\n",
    "\n",
    "    return self.columns\n",
    "\n",
    "\n",
    "@patch_to(DomoDataset_Schema)\n",
    "def remove_col(self: DomoDataset_Schema,\n",
    "               remove_col: DomoDataset_Schema_Column,\n",
    "               debug_prn: bool = False):\n",
    "\n",
    "    [self.columns.pop(index) for index, col in enumerate(self.columns) if col == remove_col]\n",
    "\n",
    "    return self.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "published_dt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DomoDataset_Schema_Column(name='objectID', id='objectID', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='url', id='url', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='Title', id='Title', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='article', id='article', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='views', id='views', type='LONG', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='created_dt', id='created_dt', type='DATETIME', order=0, visible=True, upsert_key=False)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_schema = DomoDataset_Schema()\n",
    "await ds_schema.get(auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"])\n",
    "\n",
    "old_col = ds_schema.columns[-1]\n",
    "print(old_col.name)\n",
    "ds_schema.remove_col(old_col, debug_prn= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | exporti\n",
    "@patch_to(DomoDataset_Schema)\n",
    "async def alter_schema(self: DomoDataset_Schema,\n",
    "                        dataset_id: str = None,\n",
    "                        auth: dmda.DomoAuth = None,\n",
    "                        return_raw: bool = False,\n",
    "                        debug_api: bool = False):\n",
    "    \n",
    "    dataset_id = dataset_id or self.dataset.id\n",
    "    auth = auth or self.dataset.auth\n",
    "\n",
    "    schema_obj = self.to_dict()\n",
    "\n",
    "    if return_raw:\n",
    "        return schema_obj\n",
    "    \n",
    "\n",
    "    res = await dataset_routes.alter_schema(dataset_id=dataset_id, auth = auth, schema_obj = schema_obj, debug_api = debug_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_schema = DomoDataset_Schema()\n",
    "await ds_schema.get(auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"])\n",
    "\n",
    "await ds_schema.alter_schema( auth = token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"], return_raw=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class DatasetTags_SetTagsError(Exception):\n",
    "    \"\"\"return if DatasetTags request is not successfull\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_id, domo_instance):\n",
    "        message = f\"failed to set tags on dataset - {dataset_id} in {domo_instance}\"\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DomoDataset_Tags:\n",
    "    \"\"\"class for interacting with dataset tags\"\"\"\n",
    "\n",
    "    dataset: any = None\n",
    "    tag_ls: List[str] = field(default_factory=list)\n",
    "\n",
    "    async def get(\n",
    "        self,\n",
    "        dataset_id: str = None,\n",
    "        auth: Optional[dmda.DomoAuth] = None,\n",
    "        debug_api: bool = False,\n",
    "        session: Optional[httpx.AsyncClient] = None,\n",
    "    ) -> List[str]:  # returns a list of tags\n",
    "        \"\"\"gets the existing list of dataset_tags\"\"\"\n",
    "\n",
    "        auth, dataset_id = await _have_prereqs(self = self, auth=auth, dataset_id=dataset_id, function_name=\"DomoDataset_Tages.get\")\n",
    "\n",
    "        res = await dataset_routes.get_dataset_by_id(\n",
    "            dataset_id=dataset_id, auth=auth, debug_api=debug_api, session=session\n",
    "        )\n",
    "\n",
    "        if res.is_success == False:\n",
    "            print(res)\n",
    "            return None\n",
    "\n",
    "        tag_ls = []\n",
    "\n",
    "        if res.response.get(\"tags\"):\n",
    "            tag_ls = json.loads(res.response.get(\"tags\"))\n",
    "        \n",
    "        self.tag_ls = tag_ls\n",
    "\n",
    "        return tag_ls\n",
    "\n",
    "    async def set(\n",
    "        self,\n",
    "        tag_ls: [str],\n",
    "        dataset_id: str = None,\n",
    "        auth: Optional[dmda.DomoAuth] = None,\n",
    "        debug_api: bool = False,\n",
    "        session: Optional[httpx.AsyncClient] = None,\n",
    "    ) -> List[str]: # returns a list of tags\n",
    "        \"\"\"replaces all tags with a new list of dataset_tags\"\"\"\n",
    "\n",
    "        auth, dataset_id = await _have_prereqs(self = self , auth=auth, dataset_id=dataset_id, function_name=\"DomoDatasetTags.set\")\n",
    "\n",
    "        res = await dataset_routes.set_dataset_tags(\n",
    "            auth=auth,\n",
    "            tag_ls=list(set(tag_ls)),\n",
    "            dataset_id=dataset_id,\n",
    "            debug_api=debug_api,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "        if res.status != 200:\n",
    "            raise DatasetTags_SetTagsError(\n",
    "                dataset_id=dataset_id, domo_instance=auth.domo_instance\n",
    "            )\n",
    "\n",
    "        await self.get(dataset_id=dataset_id, auth=auth)\n",
    "\n",
    "        return self.tag_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['developer_documentation', 'hackercore']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_tag = DomoDataset_Tags()\n",
    "\n",
    "await ds_tag.get(auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dec-18-2023 15:37', 'developer_documentation', 'hackercore']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "today = dt.datetime.now().strftime(\"%b-%d-%Y %H:%M\")\n",
    "\n",
    "ds_tag = DomoDataset_Tags()\n",
    "\n",
    "await ds_tag.set(\n",
    "    auth=token_auth,\n",
    "    dataset_id=os.environ[\"DOJO_DATASET_ID\"],\n",
    "    tag_ls=[\"developer_documentation\", \"hackercore\", today],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch_to(DomoDataset_Tags)\n",
    "async def add(\n",
    "    self: DomoDataset_Tags,\n",
    "    add_tag_ls: [str],\n",
    "    dataset_id: str = None,\n",
    "    auth: Optional[dmda.DomoAuth] = None,\n",
    "    debug_api: bool = False,\n",
    "    session: Optional[httpx.AsyncClient] = None,\n",
    ") -> List[str]:  # returns a list of tags\n",
    "    \"\"\"appends tags to the list of existing dataset_tags\"\"\"\n",
    "\n",
    "    auth, dataset_id = await _have_prereqs(self = self, auth=auth, dataset_id=dataset_id, function_name = \"DomoDataset_Tags.add\")\n",
    "\n",
    "    existing_tag_ls = await self.get(dataset_id=dataset_id, auth=auth) or []\n",
    "    \n",
    "    add_tag_ls += existing_tag_ls\n",
    "\n",
    "    return await self.set(\n",
    "        auth=auth,\n",
    "        dataset_id=dataset_id,\n",
    "        tag_ls=list(set(add_tag_ls)),\n",
    "        debug_api=debug_api,\n",
    "        session=session,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dec-18-2023 15:37', '2023', 'developer_documentation', 'hackercore']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "today_year = dt.datetime.today().strftime(\"%Y\")\n",
    "ds_tag = DomoDataset_Tags()\n",
    "await ds_tag.add(\n",
    "    auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"], add_tag_ls=[today_year]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(DomoDataset_Tags)\n",
    "async def remove(self: DomoDataset_Tags,\n",
    "                 remove_tag_ls: [str],\n",
    "                 dataset_id: str = None,\n",
    "                 auth: dmda.DomoFullAuth = None,\n",
    "                 debug_api: bool = False,\n",
    "                 session: Optional[httpx.AsyncClient] = None\n",
    "                 ) -> List[str]:  # returns a list of tags\n",
    "    \"\"\"removes tags from the existing list of dataset_tags\"\"\"\n",
    "\n",
    "    auth, dataset_id = await _have_prereqs(self = self, auth=auth, dataset_id=dataset_id, function_name = \"DomoDataset_Tags.remove\")\n",
    "\n",
    "    existing_tag_ls = await self.get(dataset_id=dataset_id, auth=auth)\n",
    "\n",
    "    existing_tag_ls = [\n",
    "        ex for ex in existing_tag_ls if ex not in remove_tag_ls]\n",
    "\n",
    "    return await self.set(auth=auth,\n",
    "                          dataset_id=dataset_id,\n",
    "                          tag_ls=list(set(existing_tag_ls)),\n",
    "                          debug_api=debug_api, session=session)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample implementatioin of remove tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dec-18-2023 15:37', 'developer_documentation', 'hackercore']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "today_year = dt.datetime.today().strftime(\"%Y\")\n",
    "\n",
    "ds_tag = DomoDataset_Tags()\n",
    "\n",
    "await ds_tag.remove(\n",
    "    auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"], remove_tag_ls=[ today_year])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN - Domo Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class DomoDataset:\n",
    "    \"interacts with domo datasets\"\n",
    "\n",
    "    auth: dmda.DomoAuth = field(repr=False, default=None)\n",
    "\n",
    "    id: str = \"\"\n",
    "    display_type: str = \"\"\n",
    "    data_provider_type: str = \"\"\n",
    "    name: str = \"\"\n",
    "    description: str = \"\"\n",
    "    row_count: int = None\n",
    "    column_count: int = None\n",
    "\n",
    "    stream_id: int = None\n",
    "\n",
    "    owner: dict = field(default_factory=dict)\n",
    "    formula: dict = field(default_factory=dict)\n",
    "\n",
    "    schema: DomoDataset_Schema = field(default=None)\n",
    "    tags: DomoDataset_Tags = field(default=None)\n",
    "\n",
    "    certification: dmdc.DomoCertification = None\n",
    "    PDP: dmpdp.Dataset_PDP_Policies = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.schema = DomoDataset_Schema(dataset=self)\n",
    "        self.tags = DomoDataset_Tags(dataset=self)\n",
    "\n",
    "        self.PDP = dmpdp.Dataset_PDP_Policies(dataset=self)\n",
    "\n",
    "    def display_url(self):\n",
    "        return f\"https://{self.auth.domo_instance }.domo.com/datasources/{self.id}/details/overview\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample class-based implementation of get schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>visible</th>\n",
       "      <th>order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>objectID</td>\n",
       "      <td>objectID</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>url</td>\n",
       "      <td>url</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title</td>\n",
       "      <td>Title</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article</td>\n",
       "      <td>article</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>views</td>\n",
       "      <td>views</td>\n",
       "      <td>LONG</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>created_dt</td>\n",
       "      <td>created_dt</td>\n",
       "      <td>DATETIME</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>published_dt</td>\n",
       "      <td>published_dt</td>\n",
       "      <td>DATETIME</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name            id      type  visible  order\n",
       "0      objectID      objectID    STRING     True      0\n",
       "1           url           url    STRING     True      0\n",
       "2         Title         Title    STRING     True      0\n",
       "3       article       article    STRING     True      0\n",
       "4         views         views      LONG     True      0\n",
       "5    created_dt    created_dt  DATETIME     True      0\n",
       "6  published_dt  published_dt  DATETIME     True      0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this sample returns raw response from the api\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds = DomoDataset(auth=token_auth, id=os.environ[\"DOJO_DATASET_ID\"])\n",
    "\n",
    "res = await ds.schema.get(return_raw=True)\n",
    "\n",
    "pd.DataFrame(res.get(\"tables\")[0].get(\"columns\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DomoDataset_Schema_Column(name='objectID', id='objectID', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='url', id='url', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='Title', id='Title', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='article', id='article', type='STRING', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='views', id='views', type='LONG', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='created_dt', id='created_dt', type='DATETIME', order=0, visible=True, upsert_key=False),\n",
       " DomoDataset_Schema_Column(name='published_dt', id='published_dt', type='DATETIME', order=0, visible=True, upsert_key=False)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this sample returns class-based response from the api\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds = DomoDataset(auth=token_auth, id=os.environ[\"DOJO_DATASET_ID\"])\n",
    "\n",
    "await ds.schema.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch_to(DomoDataset, cls_method=True)\n",
    "async def get_from_id(\n",
    "    cls: DomoDataset,\n",
    "    dataset_id: str,\n",
    "    auth: dmda.DomoAuth,\n",
    "    debug_api: bool = False,\n",
    "    return_raw: bool = False,\n",
    "    session: httpx.AsyncClient = None,\n",
    "    debug_num_stacks_to_drop=2,\n",
    "    parent_class :str = None\n",
    "):\n",
    "    \"\"\"retrieves dataset metadata\"\"\"\n",
    "\n",
    "    parent_class = parent_class or cls.__name__\n",
    "\n",
    "    res = await dataset_routes.get_dataset_by_id(\n",
    "        auth=auth,\n",
    "        dataset_id=dataset_id,\n",
    "        debug_api=debug_api,\n",
    "        session=session,\n",
    "        debug_num_stacks_to_drop=debug_num_stacks_to_drop,\n",
    "        parent_class=parent_class,\n",
    "    )\n",
    "\n",
    "    if return_raw:\n",
    "        return res.response\n",
    "\n",
    "    dd = util_dd.DictDot(res.response)\n",
    "    ds = cls(\n",
    "        auth=auth,\n",
    "        id=dd.id,\n",
    "        display_type=dd.displayType,\n",
    "        data_provider_type=dd.dataProviderType,\n",
    "        name=dd.name,\n",
    "        description=dd.description,\n",
    "        owner=res.response.get(\"owner\"),\n",
    "        stream_id=dd.streamId,\n",
    "        row_count=int(dd.rowCount),\n",
    "        column_count=int(dd.columnCount),\n",
    "    )\n",
    "\n",
    "    if dd.properties.formulas.formulas.__dict__:\n",
    "        # print(dd.properties.formulas.formulas.__dict__)\n",
    "        ds.formula = res.response.get(\"properties\").get(\"formulas\").get(\"formulas\")\n",
    "\n",
    "    if dd.tags:\n",
    "        ds.tags.tag_ls = json.loads(dd.tags)\n",
    "\n",
    "    if dd.certification:\n",
    "        # print('class def certification', dd.certification)\n",
    "        ds.certification = dmdc.DomoCertification._from_json(dd.certification)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample implementation of get_from_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛑  DatasetNotFoundError 🛑 - function: DomoDataset.get_from_id || dataset - 123 not found || status 404 || error at domo-community\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "try:\n",
    "    await DomoDataset.get_from_id(auth=token_auth, dataset_id=\"123\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DomoDataset(id='04c1574e-c8be-4721-9846-c6ffa491144b', display_type='domo-jupyterdata', data_provider_type='domo-jupyterdata', name='domo_kbs', description=None, row_count=1185, column_count=7, stream_id=825, owner={'id': '1893952720', 'name': 'Jae Wilson1', 'type': 'USER', 'group': False}, formula={'calculation_ca9d4b1c-f73a-4f76-9f94-d3c4ca6871c5': {'templateId': 2664, 'id': 'calculation_ca9d4b1c-f73a-4f76-9f94-d3c4ca6871c5', 'name': 'rowcount', 'formula': 'sum(1)', 'status': 'VALID', 'dataType': 'LONG', 'persistedOnDataSource': True, 'isAggregatable': True, 'bignumber': False, 'variable': False}, 'calculation_38846559-d190-4ab1-809b-bcd361db5670': {'templateId': 2665, 'id': 'calculation_38846559-d190-4ab1-809b-bcd361db5670', 'name': 'max_views', 'formula': 'max(views)', 'status': 'VALID', 'dataType': 'LONG', 'persistedOnDataSource': True, 'isAggregatable': True, 'bignumber': False, 'columnPositions': [{'columnName': 'views', 'columnPosition': 4}], 'variable': False}}, schema=DomoDataset_Schema(dataset=..., columns=[]), tags=DomoDataset_Tags(dataset=..., tag_ls=['Dec-18-2023 15:37', 'developer_documentation', 'hackercore']), certification=None, PDP=<domolibrary.classes.DomoPDP.Dataset_PDP_Policies object>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "dataset_id = os.environ[\"DOJO_DATASET_ID\"]\n",
    "\n",
    "domo_ds = await DomoDataset.get_from_id(auth=token_auth, dataset_id= dataset_id)\n",
    "\n",
    "await domo_ds.tags.get()\n",
    "\n",
    "domo_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class QueryExecutionError(de.DomoError):\n",
    "    def __init__(self,\n",
    "                 sql, dataset_id,\n",
    "                 domo_instance,\n",
    "                 status, message,\n",
    "                 function_name=None ):\n",
    "        \n",
    "        message = f\"error executing {sql}: {message}\"\n",
    "\n",
    "        super().__init__(entity_id=dataset_id,\n",
    "                         function_name=function_name,\n",
    "                         status=status,\n",
    "                         message=message,\n",
    "                         domo_instance=domo_instance)\n",
    "\n",
    "\n",
    "@patch_to(DomoDataset, cls_method=True)\n",
    "async def query_dataset_private(cls: DomoDataset,\n",
    "                                auth: dmda.DomoAuth,  # DomoFullAuth or DomoTokenAuth\n",
    "                                dataset_id: str,\n",
    "                                sql: str,\n",
    "                                session: Optional[httpx.AsyncClient] = None,\n",
    "                                filter_pdp_policy_id_ls : [int] = None, # filter by pdp policy\n",
    "                                loop_until_end: bool = False,  # retrieve all available rows\n",
    "                                \n",
    "                                limit=100,  # maximum rows to return per request.  refers to PAGINATION\n",
    "                                skip=0,\n",
    "                                maximum=100,  # equivalent to the LIMIT or TOP clause in SQL, the number of rows to return total\n",
    "                                \n",
    "                                debug_api: bool = False,\n",
    "                                debug_loop: bool = False,\n",
    "                                debug_num_stacks_to_drop : int = 2,\n",
    "                                timeout = 10, # larger API requests may require a longer response time\n",
    "                                maximum_retry : int = 5,\n",
    "                                parent_class : str = None,\n",
    "                                is_return_dataframe : bool = True\n",
    "                                ) -> pd.DataFrame:\n",
    "\n",
    "    parent_class = parent_class or cls.__name__\n",
    "    res = None\n",
    "    retry = 1\n",
    "\n",
    "    if filter_pdp_policy_id_ls and not isinstance(filter_pdp_policy_id_ls, list):\n",
    "        filter_pdp_policy_id_ls = [int(filter_pdp_policy_id_ls)]\n",
    "\n",
    "    while (not res or not res.is_success) and retry <= maximum_retry:\n",
    "        try:\n",
    "            res = await dataset_routes.query_dataset_private(auth=auth,\n",
    "                                                            dataset_id=dataset_id,\n",
    "                                                            sql=sql,\n",
    "                                                            maximum=maximum,\n",
    "                                                             filter_pdp_policy_id_ls=filter_pdp_policy_id_ls,\n",
    "                                                            skip=skip,\n",
    "                                                            limit=limit,\n",
    "                                                            loop_until_end=loop_until_end,\n",
    "                                                            session=session,\n",
    "                                                            debug_loop=debug_loop,\n",
    "                                                            debug_api=debug_api,\n",
    "                                                            timeout = timeout,\n",
    "                                                            debug_num_stacks_to_drop = debug_num_stacks_to_drop,\n",
    "                                                            parent_class = parent_class\n",
    "                                                            )\n",
    "        except dataset_routes.DatasetNotFoundError as e:\n",
    "            print(e)\n",
    "            return res\n",
    "\n",
    "        except dataset_routes.QueryRequestError as e:\n",
    "            print(e)\n",
    "            return res\n",
    "\n",
    "        except Exception as e:\n",
    "            if retry <= maximum_retry:\n",
    "                print(f\"⚠️ Error.  Attempt {retry} / {maximum_retry} - {e} - while query dataset {dataset_id} in {auth.domo_instance} with {sql}\" )\n",
    "            retry += 1\n",
    "\n",
    "    if res and not res.is_success:\n",
    "        raise QueryExecutionError(\n",
    "            status=res.status, message=res.response,\n",
    "            function_name=\"query_dataset_private\", \n",
    "            sql=sql, dataset_id=dataset_id, domo_instance=auth.domo_instance)\n",
    "    \n",
    "    if is_return_dataframe:\n",
    "        return pd.DataFrame(res.response)\n",
    "    \n",
    "    return res.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectID</th>\n",
       "      <th>url</th>\n",
       "      <th>Title</th>\n",
       "      <th>article</th>\n",
       "      <th>views</th>\n",
       "      <th>created_dt</th>\n",
       "      <th>published_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000005034</td>\n",
       "      <td>https://domo-support.domo.com/s/article/360042...</td>\n",
       "      <td>Adding Scale Markers to Your Charts</td>\n",
       "      <td>IntroDomo provides the ability to set scale ma...</td>\n",
       "      <td>50</td>\n",
       "      <td>2022-11-02T21:00:00</td>\n",
       "      <td>2022-11-02T21:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000004347</td>\n",
       "      <td>https://domo-support.domo.com/s/article/457779...</td>\n",
       "      <td>Accessing Goals Data</td>\n",
       "      <td>IntroIn Goals, you can see the overall status ...</td>\n",
       "      <td>23</td>\n",
       "      <td>2022-10-24T21:41:00</td>\n",
       "      <td>2022-10-24T22:39:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    objectID                                                url  \\\n",
       "0  000005034  https://domo-support.domo.com/s/article/360042...   \n",
       "1  000004347  https://domo-support.domo.com/s/article/457779...   \n",
       "\n",
       "                                 Title  \\\n",
       "0  Adding Scale Markers to Your Charts   \n",
       "1                 Accessing Goals Data   \n",
       "\n",
       "                                             article  views  \\\n",
       "0  IntroDomo provides the ability to set scale ma...     50   \n",
       "1  IntroIn Goals, you can see the overall status ...     23   \n",
       "\n",
       "            created_dt         published_dt  \n",
       "0  2022-11-02T21:00:00  2022-11-02T21:04:00  \n",
       "1  2022-10-24T21:41:00  2022-10-24T22:39:00  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"])\n",
    "\n",
    "dataset_id = os.environ[\"DOJO_DATASET_ID\"]\n",
    "\n",
    "res = await DomoDataset.query_dataset_private(dataset_id=dataset_id,\n",
    "                                              auth=token_auth,\n",
    "                                              loop_until_end=True,\n",
    "                                              sql=\"select * from table\",\n",
    "                                              limit=1000,\n",
    "                                              filter_pdp_policy_id_ls=[1225, 1226],\n",
    "                                              debug_api=False,\n",
    "                                              debug_loop=False,\n",
    "                                              )\n",
    "\n",
    "\n",
    "print(len(res))\n",
    "res[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "class DomoDataset_DeleteDataset_Error(de.DomoError):\n",
    "    def __init__(self,\n",
    "                 dataset_id,\n",
    "                 status, reason,\n",
    "                 domo_instance,\n",
    "                 function_name\n",
    "                 ):\n",
    "\n",
    "        super().__init__(entity_id=dataset_id,\n",
    "                         function_name=function_name,\n",
    "                         status=status,\n",
    "                         message=reason,\n",
    "                         domo_instance=domo_instance)\n",
    "\n",
    "\n",
    "@patch_to(DomoDataset)\n",
    "async def delete(self: DomoDataset,\n",
    "                 dataset_id=None,\n",
    "                 auth: dmda.DomoAuth = None,\n",
    "                 debug_api: bool = False,\n",
    "                 session: httpx.AsyncClient = None):\n",
    "\n",
    "    dataset_id = dataset_id or self.id\n",
    "    auth = auth or self.auth\n",
    "\n",
    "    res = await dataset_routes.delete(\n",
    "        auth=auth,\n",
    "        dataset_id=dataset_id,\n",
    "        debug_api=debug_api,\n",
    "        session=session)\n",
    "\n",
    "    if not res.is_success:\n",
    "        raise DomoDataset_DeleteDataset_Error(\n",
    "            dataset_id=dataset_id, \n",
    "            function_name=\"DomoDataset.delete\",\n",
    "            domo_instance=auth.domo_instance, \n",
    "            status=res.status, reason=res.response)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "@patch_to(DomoDataset)\n",
    "async def share(self: DomoDataset,\n",
    "                member,  # DomoUser or DomoGroup\n",
    "                auth: dmda.DomoAuth = None,\n",
    "                share_type: ShareDataset_AccessLevelEnum = ShareDataset_AccessLevelEnum.CAN_SHARE,\n",
    "                is_send_email=False,\n",
    "                debug_api: bool = False,\n",
    "                debug_prn: bool = False,\n",
    "                session: httpx.AsyncClient = None):\n",
    "\n",
    "    body = dataset_routes.generate_share_dataset_payload(entity_type='GROUP' if type(member).__name__ == 'DomoGroup' else 'USER',\n",
    "                                                         entity_id=int(\n",
    "                                                             member.id),\n",
    "                                                         access_level=share_type,\n",
    "                                                         is_send_email=is_send_email)\n",
    "\n",
    "    \n",
    "    res = await dataset_routes.share_dataset(auth=auth or self.auth,\n",
    "                                             dataset_id=self.id,\n",
    "                                             body=body,\n",
    "                                             session=session,\n",
    "                                             debug_api=debug_api)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseGetData(status=200, response='updated access list USER - 663516735 added to d2b21660-4ba8-400c-badf-aeef5a9abae1', is_success=True, parent_class=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import domolibrary.classes.DomoUser as dmu\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "user_id = os.environ[\"DOMO_DOJO_USER_ID\"]\n",
    "domo_user = await dmu.DomoUser.get_by_id(user_id=user_id, auth=token_auth)\n",
    "\n",
    "dataset_id = \"d2b21660-4ba8-400c-badf-aeef5a9abae1\"\n",
    "domo_dataset = await DomoDataset.get_from_id(dataset_id = dataset_id, auth = token_auth)\n",
    "\n",
    "\n",
    "await domo_dataset.share(member = domo_user, is_send_email = False, debug_api = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#     @classmethod\n",
    "#     async def query_dataset(cls,\n",
    "#                             sql: str,\n",
    "#                             dataset_id: str,\n",
    "#                             dev_auth: DomoDeveloperAuth,\n",
    "#                             debug_api: bool = False,\n",
    "#                             session: httpx.AsyncClient = None) -> pd.DataFrame:\n",
    "\n",
    "#         if debug_api:\n",
    "#             print(\"query dataset class method\")\n",
    "#             print({'dataset_id': dataset_id,\n",
    "#                    'dev_auth': dev_auth})\n",
    "\n",
    "#         res = await dataset_routes.query_dataset_public(dev_auth=dev_auth, id=dataset_id, sql=sql, session=session,\n",
    "#                                                         debug=debug)\n",
    "\n",
    "#         if debug_api:\n",
    "#             print(res.response)\n",
    "\n",
    "#         if res.status == 200:\n",
    "#             df = pd.DataFrame(data=res.response.get('rows'),\n",
    "#                               columns=res.response.get('columns'))\n",
    "#             return df\n",
    "#         return None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(DomoDataset)\n",
    "async def index_dataset(self: DomoDataset,\n",
    "                        auth: dmda.DomoAuth = None,\n",
    "                        dataset_id: str = None,\n",
    "                        debug_api: bool = False,\n",
    "                        session: httpx.AsyncClient = None\n",
    "                        ):\n",
    "\n",
    "    auth = auth or self.auth\n",
    "    dataset_id = dataset_id or self.id\n",
    "    return await dataset_routes.index_dataset(auth=auth, dataset_id=dataset_id, debug_api=debug_api,\n",
    "                                              session=session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@patch_to(DomoDataset)\n",
    "async def upload_data(self: DomoDataset,\n",
    "                      upload_df: pd.DataFrame = None,\n",
    "                      upload_df_ls: list[pd.DataFrame] = None,\n",
    "                      upload_file: io.TextIOWrapper = None,\n",
    "\n",
    "                      upload_method: str = 'REPLACE',  # APPEND or REPLACE\n",
    "                      partition_key: str = None,\n",
    "\n",
    "                      is_index: bool = True,\n",
    "\n",
    "                      dataset_id: str = None,\n",
    "                      dataset_upload_id=None,\n",
    "\n",
    "                      auth: dmda.DomoAuth = None,\n",
    "\n",
    "                      session: httpx.AsyncClient = None,\n",
    "                      debug_api: bool = False,\n",
    "                      debug_prn: bool = False\n",
    "                      ):\n",
    "\n",
    "    auth, dataset_id = await _have_prereqs(self=self, auth=auth, dataset_id=dataset_id, function_name=\"upload_data\")\n",
    "\n",
    "    upload_df_ls = upload_df_ls or [upload_df]\n",
    "\n",
    "    status_message = f\"{dataset_id} {partition_key} | {auth.domo_instance}\"\n",
    "\n",
    "    # stage 1 get uploadId\n",
    "    retry = 1\n",
    "    while dataset_upload_id is None and retry <5:\n",
    "        try:\n",
    "            if debug_prn:\n",
    "                print(f\"\\n\\n🎭 starting Stage 1 - {status_message}\")\n",
    "\n",
    "            res = await dataset_routes.upload_dataset_stage_1(auth=auth,\n",
    "                                                            dataset_id=dataset_id,\n",
    "                                                            session=session,\n",
    "                                                            partition_tag=partition_key,\n",
    "                                                            debug_api=debug_api\n",
    "                                                            )\n",
    "            if debug_prn:\n",
    "                print(\n",
    "                    f\"\\n\\n🎭 Stage 1 response -- {res.status} for {status_message}\")\n",
    "\n",
    "            dataset_upload_id = res.response\n",
    "        \n",
    "        except dataset_routes.UploadDataError as e:\n",
    "            print(f\"{e} - attempt{retry}\")\n",
    "            retry += 1\n",
    "            \n",
    "            if retry == 5:\n",
    "                print(f\"failed to upload data for {dataset_id} in {auth.domo_instance}\")\n",
    "                raise e\n",
    "                return\n",
    "            \n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "    # stage 2 upload_dataset\n",
    "    if upload_file:\n",
    "        if debug_prn:\n",
    "            print(f\"\\n\\n🎭 starting Stage 2 - upload file for {status_message}\")\n",
    "\n",
    "        res = await ce.gather_with_concurrency(n = 60, *[dataset_routes.upload_dataset_stage_2_file(auth=auth,\n",
    "                                                                                dataset_id=dataset_id,\n",
    "                                                                                upload_id=dataset_upload_id,\n",
    "                                                                                part_id=1,\n",
    "                                                                                data_file=upload_file,\n",
    "                                                                                session=session, debug_api=debug_api)])\n",
    "\n",
    "    else:\n",
    "        if debug_prn:\n",
    "            print(\n",
    "                f\"\\n\\n🎭 starting Stage 2 - {len(upload_df_ls)} - number of parts for {status_message}\")\n",
    "\n",
    "        res = await ce.gather_with_concurrency(n = 60, *[dataset_routes.upload_dataset_stage_2_df(auth=auth,\n",
    "                                                                              dataset_id=dataset_id,\n",
    "                                                                              upload_id=dataset_upload_id,\n",
    "                                                                              part_id=index + 1,\n",
    "                                                                              upload_df=df,\n",
    "                                                                              session=session, debug_api=debug_api) for index, df in enumerate(upload_df_ls)])\n",
    "\n",
    "    if debug_prn:\n",
    "        print(f\"🎭 Stage 2 - upload data: complete for {status_message}\")\n",
    "\n",
    "    # stage 3 commit_data\n",
    "    if debug_prn:\n",
    "        print(\n",
    "            f\"\\n\\n🎭 starting Stage 3 - commit dataset_upload_id for {status_message}\")\n",
    "\n",
    "    await asyncio.sleep(5)  # wait for uploads to finish\n",
    "\n",
    "    res = await dataset_routes.upload_dataset_stage_3(auth=auth,\n",
    "                                                      dataset_id=dataset_id,\n",
    "                                                      upload_id=dataset_upload_id,\n",
    "                                                      update_method=upload_method,\n",
    "                                                      partition_tag=partition_key,\n",
    "                                                      is_index=False,\n",
    "                                                      session=session,\n",
    "                                                      debug_api=debug_api)\n",
    "\n",
    "    if debug_prn:\n",
    "        print(f\"\\n🎭 stage 3 - commit dataset: complete for {status_message} \")\n",
    "\n",
    "    if is_index:\n",
    "        await asyncio.sleep(3)\n",
    "        return await self.index_dataset(auth=auth,\n",
    "                                        dataset_id=dataset_id,\n",
    "                                        debug_api=debug_api,\n",
    "                                        session=session)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(DomoDataset)\n",
    "async def list_partitions(self : DomoDataset,\n",
    "                            auth: dmda.DomoAuth = None,\n",
    "                            dataset_id: str = None,\n",
    "                            debug_api: bool = False,\n",
    "                            session: httpx.AsyncClient = None\n",
    "                            ):\n",
    "\n",
    "    auth = auth or self.auth\n",
    "    dataset_id = dataset_id or self.id\n",
    "\n",
    "    res = await dataset_routes.list_partitions(auth=auth, dataset_id=dataset_id, debug_api=debug_api,\n",
    "                                                session=session)\n",
    "    if res.status != 200:\n",
    "        return None\n",
    "\n",
    "    return res.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample implementation list_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataId</th>\n",
       "      <th>partitionId</th>\n",
       "      <th>dateCompleted</th>\n",
       "      <th>rowCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>1682630897000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>1682630897000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>1682630897000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataId partitionId  dateCompleted  rowCount\n",
       "0       2  2023-04-26  1682630897000         1\n",
       "1       3  2023-04-25  1682630897000         1\n",
       "2       4  2023-04-24  1682630897000         1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "dataset_id = 'b102e530-6472-4cc3-b420-d5ab3792fdf9'\n",
    "\n",
    "ds = await DomoDataset.get_from_id(auth=token_auth, dataset_id=dataset_id)\n",
    "ds_partition_ls = await ds.list_partitions()\n",
    "\n",
    "pd.DataFrame(ds_partition_ls[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class DomoDataset_CreateDataset_Error(Exception):\n",
    "    def __init__(self, domo_instance: str, dataset_name: str, status: int, reason: str):\n",
    "        message = f\"Failure to create dataset {dataset_name} in {domo_instance} :: {status} - {reason}\"\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "@patch_to(DomoDataset, cls_method=True)\n",
    "async def create(cls: DomoDataset,\n",
    "                 dataset_name: str,\n",
    "                 dataset_type='api',\n",
    "\n",
    "                 schema=None,\n",
    "                 auth: dmda.DomoAuth = None,\n",
    "                 debug_api: bool = False, \n",
    "                 session : httpx.AsyncClient = None\n",
    "                 ):\n",
    "    schema = schema or {\"columns\": [\n",
    "        {\"name\": 'col1', \"type\": 'LONG', \"upsertKey\": False},\n",
    "        {\"name\": 'col2', \"type\": 'STRING', \"upsertKey\": False}\n",
    "    ]}\n",
    "    \n",
    "\n",
    "    res = await dataset_routes.create(dataset_name=dataset_name,\n",
    "                                      dataset_type=dataset_type,\n",
    "                                      schema=schema, auth=auth, debug_api=debug_api, session=session\n",
    "                                      )\n",
    "\n",
    "    if not res.is_success:\n",
    "        raise DomoDataset_CreateDataset_Error(\n",
    "            domo_instance=auth.domo_instance, dataset_name=dataset_name, \n",
    "            status=res.status, reason=res.response)\n",
    "\n",
    "    dataset_id = res.response.get('dataSource').get('dataSourceId')\n",
    "\n",
    "    return await cls.get_from_id(dataset_id=dataset_id, auth=auth)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample implementation of create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "# await DomoDataset.create( dataset_name= 'Hello world_v2', dataset_type='API', auth = token_auth, debug_api = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch_to(DomoDataset)\n",
    "async def delete_partition(self: DomoDataset,\n",
    "                           dataset_partition_id: str,\n",
    "                           dataset_id: str = None,\n",
    "                           empty_df: pd.DataFrame = None,\n",
    "                           auth: dmda.DomoAuth = None,\n",
    "                           is_index: bool = True,\n",
    "                           debug_api: bool = False,\n",
    "                           debug_prn:bool = False,\n",
    "                           return_raw: bool = False\n",
    "                           ):\n",
    "\n",
    "    auth = auth or self.auth\n",
    "    dataset_id = dataset_id or self.id\n",
    "\n",
    "    if empty_df is None:\n",
    "        empty_df = await self.query_dataset_private(auth=auth,\n",
    "                                                    dataset_id=dataset_id,\n",
    "                                                    sql=\"SELECT * from table limit 1\",\n",
    "                                                    debug_api=debug_api)\n",
    "\n",
    "    await self.upload_data(upload_df=empty_df.head(0),\n",
    "                           upload_method='REPLACE',\n",
    "                           is_index=is_index,\n",
    "                           partition_key=dataset_partition_id,\n",
    "                           debug_api=debug_api)\n",
    "    if debug_prn:\n",
    "        print(f\"\\n\\n🎭 starting Stage 1\")\n",
    "\n",
    "    res = await dataset_routes.delete_partition_stage_1(auth=auth,\n",
    "                                                        dataset_id=dataset_id,\n",
    "                                                        dataset_partition_id=dataset_partition_id,\n",
    "                                                        debug_api=debug_api)\n",
    "    if debug_prn:\n",
    "        print(f\"\\n\\n🎭 Stage 1 response -- {res.status}\")\n",
    "        print(res)\n",
    "\n",
    "    if debug_prn:\n",
    "        print('starting Stage 2')\n",
    "\n",
    "    res = await dataset_routes.delete_partition_stage_2(auth=auth,\n",
    "                                                         dataset_id=dataset_id,\n",
    "                                                         dataset_partition_id=dataset_partition_id,\n",
    "                                                         debug_api=debug_api)\n",
    "    \n",
    "    if debug_prn:\n",
    "        print(f\"\\n\\n🎭 Stage 2 response -- {res.status}\")\n",
    "\n",
    "    \n",
    "    if debug_prn:\n",
    "        print('starting Stage 3')\n",
    "\n",
    "    res = await dataset_routes.index_dataset(auth=auth,\n",
    "                                                     dataset_id=dataset_id,\n",
    "                                                     debug_api=debug_api)\n",
    "    if debug_prn:\n",
    "        print(f\"\\n\\n🎭 Stage 3 response -- {res.status}\")\n",
    "\n",
    "    if return_raw:\n",
    "        return res\n",
    "\n",
    "    return res.response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample data delete_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dataId': 2, 'partitionId': '2023-04-26', 'dateCompleted': 1682630897000, 'rowCount': 1}, {'dataId': 3, 'partitionId': '2023-04-25', 'dateCompleted': 1682630897000, 'rowCount': 1}, {'dataId': 4, 'partitionId': '2023-04-24', 'dateCompleted': 1682630897000, 'rowCount': 1}]\n",
      "\n",
      "\n",
      "🎭 starting Stage 1\n",
      "\n",
      "\n",
      "🎭 Stage 1 response -- 200\n",
      "ResponseGetData(status=200, response='', is_success=True, parent_class=None)\n",
      "starting Stage 2\n",
      "\n",
      "\n",
      "🎭 Stage 2 response -- 200\n",
      "starting Stage 3\n",
      "\n",
      "\n",
      "🎭 Stage 3 response -- 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'dataId': 2,\n",
       "  'partitionId': '2023-04-26',\n",
       "  'dateCompleted': 1682630897000,\n",
       "  'rowCount': 1},\n",
       " {'dataId': 3,\n",
       "  'partitionId': '2023-04-25',\n",
       "  'dateCompleted': 1682630897000,\n",
       "  'rowCount': 1},\n",
       " {'dataId': 4,\n",
       "  'partitionId': '2023-04-24',\n",
       "  'dateCompleted': 1682630897000,\n",
       "  'rowCount': 1}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "dataset_id = 'b102e530-6472-4cc3-b420-d5ab3792fdf9'\n",
    "dataset = await DomoDataset.get_from_id(auth=token_auth, dataset_id=dataset_id)\n",
    "\n",
    "ds_partition_ls = await dataset.list_partitions()\n",
    "print(ds_partition_ls)\n",
    "\n",
    "ds = await dataset.delete_partition(\n",
    "    dataset_partition_id=\"2023-04-27\",\n",
    "    auth=token_auth, \n",
    "    dataset_id=dataset_id,\n",
    "    debug_api = False,\n",
    "    debug_prn = True)\n",
    "\n",
    "await dataset.list_partitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@patch_to(DomoDataset)\n",
    "async def reset_dataset(self: DomoDataset,\n",
    "                        auth: dmda.DomoAuth = None,\n",
    "                        is_index: bool = True,\n",
    "                        debug_api: bool = False\n",
    "                        ):\n",
    "\n",
    "    execute_reset = input(\n",
    "        \"This function will delete all rows.  Type BLOW_ME_AWAY to execute:\")\n",
    "\n",
    "    if execute_reset != 'BLOW_ME_AWAY':\n",
    "        print(\"You didn't type BLOW_ME_AWAY, moving on.\")\n",
    "        return None\n",
    "\n",
    "    auth = auth or self.auth\n",
    "\n",
    "    if not auth:\n",
    "        raise Exception(\"auth required\")\n",
    "\n",
    "    # create empty dataset to retain schema\n",
    "    empty_df = await self.query_dataset_private(auth=auth,\n",
    "                                                dataset_id=self.id,\n",
    "                                                sql=\"SELECT * from table limit 1\",\n",
    "                                                debug_api =debug_api)\n",
    "    empty_df = empty_df.head(0)\n",
    "\n",
    "    # get partition list\n",
    "    partition_list = await self.list_partitions()\n",
    "    if len(partition_list) > 0:\n",
    "        partition_list = ce.chunk_list(partition_list, 100)\n",
    "\n",
    "    for index, pl in enumerate(partition_list):\n",
    "        print(f'🥫 starting chunk {index + 1} of {len(partition_list)}')\n",
    "\n",
    "        await asyncio.gather(*[self.delete_partition(auth=auth,\n",
    "                                                    dataset_partition_id=partition.get('partitionId'),\n",
    "                                                    empty_df=empty_df,\n",
    "                                                    debug_api =debug_api) for partition in pl])\n",
    "        if is_index:\n",
    "            await self.index_dataset()\n",
    "\n",
    "    res = await self.upload_data(upload_df=empty_df,\n",
    "                                upload_method='REPLACE',\n",
    "                                is_index=is_index,\n",
    "                                debug_api=debug_api)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
