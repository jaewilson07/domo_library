{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> a class based approach for interacting with Domo Datasets\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: dataset_class.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp classes.DomoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from fastcore.basics import patch_to\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "import json\n",
    "import io\n",
    "\n",
    "import httpx\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# import datetime as dt\n",
    "\n",
    "# import importlib\n",
    "# import json\n",
    "# from pprint import pprint\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# from ..utils.Base import Base\n",
    "# from ..utils.chunk_execution import chunk_list\n",
    "# from . import DomoCertification as dmdc\n",
    "# from . import DomoPDP as dmpdp\n",
    "# from . import DomoTag as dmtg\n",
    "\n",
    "\n",
    "import domolibrary.utils.DictDot as util_dd\n",
    "import domolibrary.client.DomoAuth as dmda\n",
    "import domolibrary.client.DomoError as de\n",
    "import domolibrary.routes.dataset as dataset_routes\n",
    "#import domolibrary.classes.DomoPDP as dmpdp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Classes\n",
    "## DatasetSchema\n",
    "\n",
    "The `DomoDataset_Schema` class will be a subclass of `DomoDataset`. It will handle all of the methods for interacting with schemas.\n",
    "\n",
    "- In execution, the schema is separate from the data that gets uploaded from Vault to Adrenaline. The domo schema defines how the data is loaded into Vault.\n",
    "- Be cognizant to match dataset uploads with schema definitions. If the schema and uploaded data types do not match, the dataset may be unable to index in Adrenaline (and therefore not update).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "async def _have_prereqs(self, auth, dataset_id, function_name):\n",
    "    \"\"\"tests if have a parent dataset or prerequsite dataset_id and auth object\"\"\"\n",
    "\n",
    "    auth_from_self_dataset = getattr(self.dataset, 'auth', None) if getattr(self, 'dataset', None) else None\n",
    "    auth_from_self = getattr(self , 'auth', None)\n",
    "\n",
    "    auth = auth or auth_from_self or auth_from_self_dataset\n",
    "\n",
    "    await auth.get_auth_token()\n",
    "\n",
    "    if not auth or not auth.token:\n",
    "        raise de.AuthNotProvidedError(\n",
    "            function_name=function_name,\n",
    "            entity_id = self.dataset.id)\n",
    "\n",
    "    id_from_self = getattr(self, 'id', None)\n",
    "    id_from_self_parent = getattr(self.dataset, 'id', None ) if getattr(self, 'dataset', None) else None\n",
    "    \n",
    "    dataset_id = dataset_id or id_from_self or id_from_self_parent\n",
    "    \n",
    "    if not dataset_id:\n",
    "        raise de.DatasetNotProvidedError(\n",
    "            function_name = function_name, \n",
    "            domo_instance = auth.domo_instance\n",
    "        )\n",
    "\n",
    "    return auth, dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class DatasetSchema_Types(Enum):\n",
    "    STRING = 'STRING'\n",
    "    DOUBLE = 'DOUBLE'\n",
    "    LONG = 'LONG'\n",
    "    DATE = 'DATE'\n",
    "    DATETIME = 'DATETIME'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DomoDataset_Schema_Column:\n",
    "    name: str\n",
    "    id: str\n",
    "    type: DatasetSchema_Types\n",
    "\n",
    "    @classmethod\n",
    "    def _from_json(cls, json_obj):\n",
    "        dd = util_dd.DictDot(json_obj)\n",
    "        return cls(name=dd.name, id=dd.id, type=dd.type)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DomoDataset_Schema:\n",
    "    \"\"\"class for interacting with dataset schemas\"\"\"\n",
    "\n",
    "    dataset: any = None\n",
    "    columns: List[DomoDataset_Schema_Column] = field(default_factory=list)\n",
    "\n",
    "    async def get(\n",
    "        self,\n",
    "        auth: Optional[dmda.DomoAuth] = None,\n",
    "        dataset_id: str = None,\n",
    "        debug_api: bool = False,\n",
    "        return_raw_res: bool = False,  # return the raw response\n",
    "    ) -> List[DomoDataset_Schema_Column]:\n",
    "\n",
    "        \"\"\"method that retrieves schema for a dataset\"\"\"\n",
    "\n",
    "        auth, dataset_id = await _have_prereqs(self = self, auth = auth, dataset_id = dataset_id, function_name = \"DomoDataset_Schema.get\")\n",
    "\n",
    "        res = await dataset_routes.get_schema(\n",
    "            auth=auth, dataset_id=dataset_id, debug_api=debug_api\n",
    "        )\n",
    "\n",
    "        if return_raw_res:\n",
    "            return res.response\n",
    "\n",
    "        if res.status == 200:\n",
    "            json_list = res.response.get(\"tables\")[0].get(\"columns\")\n",
    "\n",
    "            self.columns = [\n",
    "                DomoDataset_Schema_Column._from_json(json_obj=json_obj)\n",
    "                for json_obj in json_list\n",
    "            ]\n",
    "\n",
    "            return self.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/jaewilson07/domo_library/blob/main/domolibrary/classes/DomoDataset.py#L104){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DomoDataset_Schema.get\n",
       "\n",
       ">      DomoDataset_Schema.get\n",
       ">                              (auth:Optional[domolibrary.client.DomoAuth.DomoAu\n",
       ">                              th]=None, dataset_id:str=None,\n",
       ">                              debug_api:bool=False, return_raw_res:bool=False)\n",
       "\n",
       "method that retrieves schema for a dataset"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/jaewilson07/domo_library/blob/main/domolibrary/classes/DomoDataset.py#L104){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DomoDataset_Schema.get\n",
       "\n",
       ">      DomoDataset_Schema.get\n",
       ">                              (auth:Optional[domolibrary.client.DomoAuth.DomoAu\n",
       ">                              th]=None, dataset_id:str=None,\n",
       ">                              debug_api:bool=False, return_raw_res:bool=False)\n",
       "\n",
       "method that retrieves schema for a dataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DomoDataset_Schema.get)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample implementation of getting a dataset schema\n",
    "\n",
    "Standard implementation will be to access the `DomoDataset_Schema` class as the `DomoDataset.schema` property\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DomoDataset_Schema_Column(name='Dataset ID', id='Dataset ID', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='Name', id='Name', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='Description', id='Description', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='Row Count', id='Row Count', type='DOUBLE'),\n",
       " DomoDataset_Schema_Column(name='Column Count', id='Column Count', type='DOUBLE'),\n",
       " DomoDataset_Schema_Column(name='Owner ID', id='Owner ID', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='Owner Name', id='Owner Name', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='Dataset Created Date/Time', id='Dataset Created Date/Time', type='DATETIME'),\n",
       " DomoDataset_Schema_Column(name='DataSet Last Touched Date/Time', id='DataSet Last Touched Date/Time', type='DATETIME'),\n",
       " DomoDataset_Schema_Column(name='DataSet Last Updated Date/Time', id='DataSet Last Updated Date/Time', type='DATETIME'),\n",
       " DomoDataset_Schema_Column(name='Report Last Run', id='Report Last Run', type='DATETIME'),\n",
       " DomoDataset_Schema_Column(name='Type', id='Type', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='Display ProcessingType', id='Display ProcessingType', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='Data Provider ProcessingType', id='Data Provider ProcessingType', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='Card Count', id='Card Count', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='PDP Enabled', id='PDP Enabled', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='_BATCH_ID_', id='_BATCH_ID_', type='DOUBLE'),\n",
       " DomoDataset_Schema_Column(name='_BATCH_LAST_RUN_', id='_BATCH_LAST_RUN_', type='DATETIME')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_schema = DomoDataset_Schema()\n",
    "\n",
    "await ds_schema.get(auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class DatasetTags_SetTagsError(Exception):\n",
    "    \"\"\"return if DatasetTags request is not successfull\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_id, domo_instance):\n",
    "        message = f\"failed to set tags on dataset - {dataset_id} in {domo_instance}\"\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DomoDataset_Tags:\n",
    "    \"\"\"class for interacting with dataset tags\"\"\"\n",
    "\n",
    "    dataset: any = None\n",
    "    tag_ls: List[str] = field(default_factory=list)\n",
    "\n",
    "    async def get(\n",
    "        self,\n",
    "        dataset_id: str = None,\n",
    "        auth: Optional[dmda.DomoAuth] = None,\n",
    "        debug_api: bool = False,\n",
    "        session: Optional[httpx.AsyncClient] = None,\n",
    "    ) -> List[str]:  # returns a list of tags\n",
    "        \"\"\"gets the existing list of dataset_tags\"\"\"\n",
    "\n",
    "        auth, dataset_id = await _have_prereqs(self = self, auth=auth, dataset_id=dataset_id, function_name=\"DomoDataset_Tages.get\")\n",
    "\n",
    "        res = await dataset_routes.get_dataset_by_id(\n",
    "            dataset_id=dataset_id, auth=auth, debug_api=debug_api, session=session\n",
    "        )\n",
    "\n",
    "        if res.is_success == False:\n",
    "            print(res)\n",
    "            return None\n",
    "\n",
    "        tag_ls = []\n",
    "\n",
    "        if res.response.get(\"tags\"):\n",
    "            tag_ls = json.loads(res.response.get(\"tags\"))\n",
    "        \n",
    "        self.tag_ls = tag_ls\n",
    "\n",
    "        return tag_ls\n",
    "\n",
    "    async def set(\n",
    "        self,\n",
    "        tag_ls: [str],\n",
    "        dataset_id: str = None,\n",
    "        auth: Optional[dmda.DomoAuth] = None,\n",
    "        debug_api: bool = False,\n",
    "        session: Optional[httpx.AsyncClient] = None,\n",
    "    ) -> List[str]: # returns a list of tags\n",
    "        \"\"\"replaces all tags with a new list of dataset_tags\"\"\"\n",
    "\n",
    "        auth, dataset_id = await _have_prereqs(self = self , auth=auth, dataset_id=dataset_id, function_name=\"DomoDatasetTags.set\")\n",
    "\n",
    "        res = await dataset_routes.set_dataset_tags(\n",
    "            auth=auth,\n",
    "            tag_ls=list(set(tag_ls)),\n",
    "            dataset_id=dataset_id,\n",
    "            debug_api=debug_api,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "        if res.status != 200:\n",
    "            raise DatasetTags_SetTagsError(\n",
    "                dataset_id=dataset_id, domo_instance=auth.domo_instance\n",
    "            )\n",
    "\n",
    "        await self.get(dataset_id=dataset_id, auth=auth)\n",
    "\n",
    "        return self.tag_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['domostats']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x1dD\\xe3\\x8e\\xc7M\\x17OEYf\\x91)\\xf5\\xe8\\x83\"R 8&\\xa3(\\x19\\x83m\\xdeTAQ\\xa9\\xcd\\xbam\\xb7\\xef\\xed\\xd4\\x1e<\\x82\\x17i\\xe9\\xfc\\xbd\\x07\\x8f\\xfe\\x19\\xcb\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00', b'\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18']\n",
      "Bad pipe message: %s [b\"\\x06\\t'j\\xc9\\xf2\\xa58\\x8f\\x9e\\x89\\xfd\\xa8\\xe7\\xd1@(b \\xd4\\x0b=Lz\\xc8`]\\xb5\\x8b\\xf3\\xf7\\x0e\\xd0,\\xbf&\\xfb_E\\xa6h\\xac\\xee\\xa0x`ff\\xa6*\\xe3\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t1\"]\n",
      "Bad pipe message: %s [b'.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04', b'\\x03\\x06', b'\\x07\\x08']\n",
      "Bad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 z\\xcf\\x92\\xe7\\xd7\\xc1\\xb4\\xd0B\\x17\\x15=\\xd3\\xee\\xfdF2B\\x17v?\\xe9']\n",
      "Bad pipe message: %s [b\"\\xabw@V[F2\\xf0\\x8c\\xa5N\\xfb\\xfc\\x01,\\x19]\\x0b\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\"]\n",
      "Bad pipe message: %s [b\"g\\xa8\\x13E\\x81\\xcb\\xa9 \\x99j\\xbf\\xd2\\xd9_\\x12\\xd3Q\\xe4\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\"]\n",
      "Bad pipe message: %s [b'\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\\x02\\x04\\x02\\x05']\n",
      "Bad pipe message: %s [b'\\x02']\n",
      "Bad pipe message: %s [b'm\\xf3\\xc7;\\xa1o\\xe3\\xc9w\\x11`I\\xb8g\\x8e']\n",
      "Bad pipe message: %s [b'\\x8bP\\xbb\\xcd\\x05\\xd3\\x960u\\x85\\x8d\\xd4\\x0c\\xee\\xb6\\x04>\\xb2\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00', b'\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01']\n",
      "Bad pipe message: %s [b't\\xd4\\xb5\\xb58\\xfea\\x8f\\xd4Z\\xbb\\x02\\xd3\\x89v\\xa1\\xc9\"\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00']\n",
      "Bad pipe message: %s [b'\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c']\n",
      "Bad pipe message: %s [b'\\x18\\x0f\\xdeV\\x9e\\x8c\\x8c\\xf8\\x92\\x02\\xc3\\xcdS\\xb5,\\xa0C?\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x00', b\"8\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x00\"]\n",
      "Bad pipe message: %s [b'0\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds_tag = DomoDataset_Tags()\n",
    "\n",
    "await ds_tag.get(auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apr-27-2023 08:02', 'developer_documentation', 'hackercore']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "today = dt.datetime.now().strftime(\"%b-%d-%Y %H:%M\")\n",
    "\n",
    "ds_tag = DomoDataset_Tags()\n",
    "\n",
    "await ds_tag.set(\n",
    "    auth=token_auth,\n",
    "    dataset_id=os.environ[\"DOJO_DATASET_ID\"],\n",
    "    tag_ls=[\"developer_documentation\", \"hackercore\", today],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch_to(DomoDataset_Tags)\n",
    "async def add(\n",
    "    self: DomoDataset_Tags,\n",
    "    add_tag_ls: [str],\n",
    "    dataset_id: str = None,\n",
    "    auth: Optional[dmda.DomoAuth] = None,\n",
    "    debug_api: bool = False,\n",
    "    session: Optional[httpx.AsyncClient] = None,\n",
    ") -> List[str]:  # returns a list of tags\n",
    "    \"\"\"appends tags to the list of existing dataset_tags\"\"\"\n",
    "\n",
    "    auth, dataset_id = await _have_prereqs(self = self, auth=auth, dataset_id=dataset_id, function_name = \"DomoDataset_Tags.add\")\n",
    "\n",
    "    existing_tag_ls = await self.get(dataset_id=dataset_id, auth=auth) or []\n",
    "    \n",
    "    add_tag_ls += existing_tag_ls\n",
    "\n",
    "    return await self.set(\n",
    "        auth=auth,\n",
    "        dataset_id=dataset_id,\n",
    "        tag_ls=list(set(add_tag_ls)),\n",
    "        debug_api=debug_api,\n",
    "        session=session,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023', 'developer_documentation', 'hackercore', 'Apr-04-2023 22:22']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "today_year = dt.datetime.today().strftime(\"%Y\")\n",
    "ds_tag = DomoDataset_Tags()\n",
    "await ds_tag.add(\n",
    "    auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"], add_tag_ls=[today_year]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(DomoDataset_Tags)\n",
    "async def remove(self: DomoDataset_Tags,\n",
    "                 remove_tag_ls: [str],\n",
    "                 dataset_id: str = None,\n",
    "                 auth: dmda.DomoFullAuth = None,\n",
    "                 debug_api: bool = False,\n",
    "                 session: Optional[httpx.AsyncClient] = None\n",
    "                 ) -> List[str]:  # returns a list of tags\n",
    "    \"\"\"removes tags from the existing list of dataset_tags\"\"\"\n",
    "\n",
    "    auth, dataset_id = await _have_prereqs(self = self, auth=auth, dataset_id=dataset_id, function_name = \"DomoDataset_Tags.remove\")\n",
    "\n",
    "    existing_tag_ls = await self.get(dataset_id=dataset_id, auth=auth)\n",
    "\n",
    "    existing_tag_ls = [\n",
    "        ex for ex in existing_tag_ls if ex not in remove_tag_ls]\n",
    "\n",
    "    return await self.set(auth=auth,\n",
    "                          dataset_id=dataset_id,\n",
    "                          tag_ls=list(set(existing_tag_ls)),\n",
    "                          debug_api=debug_api, session=session)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample implementatioin of remove tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['developer_documentation', 'hackercore', 'Apr-04-2023 22:22']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "today_year = dt.datetime.today().strftime(\"%Y\")\n",
    "\n",
    "ds_tag = DomoDataset_Tags()\n",
    "\n",
    "await ds_tag.remove(\n",
    "    auth=token_auth, dataset_id=os.environ[\"DOJO_DATASET_ID\"], remove_tag_ls=[ today_year])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN - Domo Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dataclass\n",
    "class DomoDataset:\n",
    "    \"interacts with domo datasets\"\n",
    "\n",
    "    auth: dmda.DomoAuth = field(repr=False, default=None)\n",
    "\n",
    "    id: str = \"\"\n",
    "    display_type: str = \"\"\n",
    "    data_provider_type: str = \"\"\n",
    "    name: str = \"\"\n",
    "    description: str = \"\"\n",
    "    row_count: int = None\n",
    "    column_count: int = None\n",
    "\n",
    "    stream_id: int = None\n",
    "\n",
    "    owner: dict = field(default_factory=dict)\n",
    "    formula: dict = field(default_factory=dict)\n",
    "\n",
    "    schema: DomoDataset_Schema = field(default=None)\n",
    "    tags: DomoDataset_Tags = field(default=None)\n",
    "\n",
    "    # certification: dmdc.DomoCertification = None\n",
    "    # PDPPolicies: dmpdp.Dataset_PDP_Policies = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.schema = DomoDataset_Schema(dataset=self)\n",
    "        self.tags = DomoDataset_Tags(dataset=self)\n",
    "\n",
    "        #self.PDPPolicies = dmpdp.Dataset_PDP_Policies(dataset=self)\n",
    "\n",
    "    def display_url(self):\n",
    "        return f\"https://{self.auth.domo_instance }.domo.com/datasources/{self.id}/details/overview\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample class-based implementation of get schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>visible</th>\n",
       "      <th>order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>objectID</td>\n",
       "      <td>objectID</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>url</td>\n",
       "      <td>url</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title</td>\n",
       "      <td>Title</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article</td>\n",
       "      <td>article</td>\n",
       "      <td>STRING</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>views</td>\n",
       "      <td>views</td>\n",
       "      <td>LONG</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>created_dt</td>\n",
       "      <td>created_dt</td>\n",
       "      <td>DATETIME</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>published_dt</td>\n",
       "      <td>published_dt</td>\n",
       "      <td>DATETIME</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name            id      type  visible  order\n",
       "0      objectID      objectID    STRING     True      0\n",
       "1           url           url    STRING     True      0\n",
       "2         Title         Title    STRING     True      0\n",
       "3       article       article    STRING     True      0\n",
       "4         views         views      LONG     True      0\n",
       "5    created_dt    created_dt  DATETIME     True      0\n",
       "6  published_dt  published_dt  DATETIME     True      0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this sample returns raw response from the api\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds = DomoDataset(auth=token_auth, id=os.environ[\"DOJO_DATASET_ID\"])\n",
    "\n",
    "raw_res = await ds.schema.get(return_raw_res=True)\n",
    "\n",
    "pd.DataFrame(raw_res.get(\"tables\")[0].get(\"columns\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DomoDataset_Schema_Column(name='objectID', id='objectID', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='url', id='url', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='Title', id='Title', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='article', id='article', type='STRING'),\n",
       " DomoDataset_Schema_Column(name='views', id='views', type='LONG'),\n",
       " DomoDataset_Schema_Column(name='created_dt', id='created_dt', type='DATETIME'),\n",
       " DomoDataset_Schema_Column(name='published_dt', id='published_dt', type='DATETIME')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this sample returns class-based response from the api\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "ds = DomoDataset(auth=token_auth, id=os.environ[\"DOJO_DATASET_ID\"])\n",
    "\n",
    "await ds.schema.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch_to(DomoDataset, cls_method=True)\n",
    "async def get_from_id(\n",
    "    cls: DomoDataset,\n",
    "    dataset_id: str,\n",
    "    auth: dmda.DomoAuth,\n",
    "    debug_api: bool = False,\n",
    "    return_raw_res: bool = False,\n",
    "    session : httpx.AsyncClient = None,\n",
    "):\n",
    "\n",
    "    \"\"\"retrieves dataset metadata\"\"\"\n",
    "\n",
    "    res = await dataset_routes.get_dataset_by_id(\n",
    "        auth=auth, dataset_id=dataset_id, debug_api=debug_api, session = session\n",
    "    )\n",
    "\n",
    "    if return_raw_res:\n",
    "        return res.response\n",
    "\n",
    "    dd = util_dd.DictDot(res.response)\n",
    "    ds = cls(\n",
    "        auth=auth,\n",
    "        id=dd.id,\n",
    "        display_type=dd.displayType,\n",
    "        data_provider_type=dd.dataProviderType,\n",
    "        name=dd.name,\n",
    "        description=dd.description,\n",
    "        owner=res.response.get('owner'),\n",
    "        stream_id=dd.streamId,\n",
    "        row_count=int(dd.rowCount),\n",
    "        column_count=int(dd.columnCount),\n",
    "    )\n",
    "    \n",
    "    if dd.properties.formulas.formulas.__dict__ :\n",
    "        # print(dd.properties.formulas.formulas.__dict__)\n",
    "        ds.formula=res.response.get('properties').get('formulas').get('formulas')\n",
    "\n",
    "    if dd.tags:\n",
    "        ds.tags.tag_ls = json.loads(dd.tags)\n",
    "\n",
    "    # if dd.certification:\n",
    "    #     # print('class def certification', dd.certification)\n",
    "    #     ds.certification = dmdc.DomoCertification._from_json(\n",
    "    #         dd.certification)\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample implementation of get_from_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset - 123 not found in domo-community\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "try:\n",
    "    await DomoDataset.get_from_id(auth=token_auth, dataset_id=\"123\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DomoDataset(id='04c1574e-c8be-4721-9846-c6ffa491144b',\n",
      "            display_type='domo-jupyterdata',\n",
      "            data_provider_type='domo-jupyterdata',\n",
      "            name='domo_kbs',\n",
      "            description=None,\n",
      "            row_count=1185,\n",
      "            column_count=7,\n",
      "            stream_id=825,\n",
      "            owner={'group': False,\n",
      "                   'id': '1893952720',\n",
      "                   'name': 'Jae Wilson1',\n",
      "                   'type': 'USER'},\n",
      "            formula={'calculation_38846559-d190-4ab1-809b-bcd361db5670': {'bignumber': False,\n",
      "                                                                          'columnPositions': [{'columnName': 'views',\n",
      "                                                                                               'columnPosition': 4}],\n",
      "                                                                          'dataType': 'LONG',\n",
      "                                                                          'formula': 'max(views)',\n",
      "                                                                          'id': 'calculation_38846559-d190-4ab1-809b-bcd361db5670',\n",
      "                                                                          'isAggregatable': True,\n",
      "                                                                          'name': 'max_views',\n",
      "                                                                          'persistedOnDataSource': True,\n",
      "                                                                          'status': 'VALID',\n",
      "                                                                          'templateId': 2665},\n",
      "                     'calculation_ca9d4b1c-f73a-4f76-9f94-d3c4ca6871c5': {'bignumber': False,\n",
      "                                                                          'dataType': 'LONG',\n",
      "                                                                          'formula': 'sum(1)',\n",
      "                                                                          'id': 'calculation_ca9d4b1c-f73a-4f76-9f94-d3c4ca6871c5',\n",
      "                                                                          'isAggregatable': True,\n",
      "                                                                          'name': 'rowcount',\n",
      "                                                                          'persistedOnDataSource': True,\n",
      "                                                                          'status': 'VALID',\n",
      "                                                                          'templateId': 2664}},\n",
      "            schema=DomoDataset_Schema(dataset=...,\n",
      "                                      columns=[]),\n",
      "            tags=DomoDataset_Tags(dataset=...,\n",
      "                                  tag_ls=['developer_documentation',\n",
      "                                          'hackercore',\n",
      "                                          'Apr-04-2023 22:22']))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "dataset_id = os.environ[\"DOJO_DATASET_ID\"]\n",
    "# dataset_id = 'da552832-c04d-46ac-936a-f982d9d3f2e6'\n",
    "\n",
    "res = await DomoDataset.get_from_id(auth=token_auth, dataset_id= dataset_id)\n",
    "\n",
    "await res.tags.get()\n",
    "\n",
    "from pprint import pprint\n",
    "# pd.DataFrame([res])\n",
    "\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class QueryExecutionError(de.DomoError):\n",
    "    def __init__(self,\n",
    "                 sql, dataset_id,\n",
    "                 domo_instance,\n",
    "                 status, message,\n",
    "                 function_name=None ):\n",
    "        \n",
    "        self.message = f\"error executing {sql}: {message}\"\n",
    "\n",
    "        super().__init__(entity_id=dataset_id,\n",
    "                         function_name=function_name,\n",
    "                         status=status,\n",
    "                         message=message,\n",
    "                         domo_instance=domo_instance)\n",
    "\n",
    "\n",
    "@patch_to(DomoDataset, cls_method=True)\n",
    "async def query_dataset_private(cls: DomoDataset,\n",
    "                                auth: dmda.DomoAuth,  # DomoFullAuth or DomoTokenAuth\n",
    "                                dataset_id: str,\n",
    "                                sql: str,\n",
    "                                session: Optional[httpx.AsyncClient] = None,\n",
    "                                loop_until_end: bool = False,  # retrieve all available rows\n",
    "                                limit=100,  # maximum rows to return per request.  refers to PAGINATION\n",
    "                                skip=0,\n",
    "                                maximum=100,  # equivalent to the LIMIT or TOP clause in SQL, the number of rows to return total\n",
    "                                debug_api: bool = False,\n",
    "                                debug_loop: bool = False,\n",
    "                                timeout = 10 # larger API requests may require a longer response time\n",
    "                                ) -> pd.DataFrame:\n",
    "\n",
    "    res = await dataset_routes.query_dataset_private(auth=auth,\n",
    "                                                     dataset_id=dataset_id,\n",
    "                                                     sql=sql,\n",
    "                                                     maximum=maximum,\n",
    "                                                     skip=skip,\n",
    "                                                     limit=limit,\n",
    "                                                     loop_until_end=loop_until_end,\n",
    "                                                     session=session,\n",
    "                                                     debug_loop=debug_loop,\n",
    "                                                     debug_api=debug_api,\n",
    "                                                     timeout = timeout\n",
    "                                                     )\n",
    "\n",
    "    if not res.is_success:\n",
    "        raise QueryExecutionError(\n",
    "            status=res.status, message=res.response,\n",
    "            function_name=\"query_dataset_private\", \n",
    "            sql=sql, dataset_id=dataset_id, domo_instance=auth.domo_instance)\n",
    "\n",
    "    return pd.DataFrame(res.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class DomoDataset_DeleteDataset_Error(de.DomoError):\n",
    "    def __init__(self,\n",
    "                 dataset_id,\n",
    "                 status, reason,\n",
    "                 domo_instance,\n",
    "                 function_name\n",
    "                 ):\n",
    "\n",
    "        super().__init__(entity_id=dataset_id,\n",
    "                         function_name=function_name,\n",
    "                         status=status,\n",
    "                         message=reason,\n",
    "                         domo_instance=domo_instance)\n",
    "\n",
    "\n",
    "@patch_to(DomoDataset)\n",
    "async def delete(self: DomoDataset,\n",
    "                 dataset_id=None,\n",
    "                 auth: dmda.DomoAuth = None,\n",
    "                 debug_api: bool = False,\n",
    "                 session: httpx.AsyncClient = None):\n",
    "\n",
    "    dataset_id = dataset_id or self.id\n",
    "    auth = auth or self.auth\n",
    "\n",
    "    res = await dataset_routes.delete(\n",
    "        auth=auth,\n",
    "        dataset_id=dataset_id,\n",
    "        debug_api=debug_api,\n",
    "        session=session)\n",
    "\n",
    "    if not res.is_success:\n",
    "        raise DomoDataset_DeleteDataset_Error(\n",
    "            dataset_id=dataset_id, \n",
    "            function_name=\"DomoDataset.delete\",\n",
    "            domo_instance=auth.domo_instance, \n",
    "            status=res.status, reason=res.response)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     @classmethod\n",
    "#     async def query_dataset(cls,\n",
    "#                             sql: str,\n",
    "#                             dataset_id: str,\n",
    "#                             dev_auth: DomoDeveloperAuth,\n",
    "#                             debug_api: bool = False,\n",
    "#                             session: httpx.AsyncClient = None) -> pd.DataFrame:\n",
    "\n",
    "#         if debug_api:\n",
    "#             print(\"query dataset class method\")\n",
    "#             print({'dataset_id': dataset_id,\n",
    "#                    'dev_auth': dev_auth})\n",
    "\n",
    "#         res = await dataset_routes.query_dataset_public(dev_auth=dev_auth, id=dataset_id, sql=sql, session=session,\n",
    "#                                                         debug=debug)\n",
    "\n",
    "#         if debug_api:\n",
    "#             print(res.response)\n",
    "\n",
    "#         if res.status == 200:\n",
    "#             df = pd.DataFrame(data=res.response.get('rows'),\n",
    "#                               columns=res.response.get('columns'))\n",
    "#             return df\n",
    "#         return None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class DomoDataset_UploadData_Error(Exception):\n",
    "\n",
    "    def __init__(self,\n",
    "                 message_error: str,\n",
    "                 domo_instance: str,\n",
    "                 dataset_id: str,\n",
    "                 stage: int,\n",
    "                 status=\"\", reason=\"\",\n",
    "                 partition_key: str = None):\n",
    "\n",
    "        message_start = f\"Stage {stage}:: {message_error} :: API {status} - {reason} :: \"\n",
    "        message_end = f\"in {dataset_id} in {domo_instance}\"\n",
    "\n",
    "        message_partition = \"\"\n",
    "        if partition_key:\n",
    "            message_partition = f\"for partition - '{partition_key}' \"\n",
    "\n",
    "        message = f\"{message_start}{message_partition}{message_end}\"\n",
    "\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "class DomoDataset_UploadData_DatasetUploadId_Error(DomoDataset_UploadData_Error):\n",
    "    def __init__(self, domo_instance: str, dataset_id: str,\n",
    "                 stage: int = 1, status=\"\", reason=\"\",\n",
    "                 partition_key: str = None):\n",
    "\n",
    "        message_error = \"unable to retrieve dataset_upload_id\"\n",
    "\n",
    "        super().__init__(message_error=message_error,\n",
    "                         domo_instance=domo_instance, dataset_id=dataset_id,\n",
    "                         stage=stage, status=status, reason=reason,\n",
    "                         partition_key=partition_key)\n",
    "\n",
    "\n",
    "class DomoDataset_UploadData_UploadData_Error(DomoDataset_UploadData_Error):\n",
    "    def __init__(self, domo_instance: str, dataset_id: str,\n",
    "                 stage: int = 2, status=\"\", reason=\"\",\n",
    "                 partition_key: str = None):\n",
    "\n",
    "        message_error = \"while uploading data\"\n",
    "\n",
    "        super().__init__(message_error=message_error,\n",
    "                         domo_instance=domo_instance, dataset_id=dataset_id,\n",
    "                         stage=stage, status=status, reason=reason,\n",
    "                         partition_key=partition_key)\n",
    "\n",
    "class DomoDataset_UploadData_CommitDatasetUploadId_Error(DomoDataset_UploadData_Error):\n",
    "    def __init__(self, domo_instance: str, dataset_id: str,\n",
    "                    stage: int = 3, status=\"\", reason=\"\",\n",
    "                    partition_key: str = None):\n",
    "\n",
    "        message_error = \"while commiting dataset_upload_id\"\n",
    "\n",
    "        super().__init__(message_error=message_error,\n",
    "                            domo_instance=domo_instance, dataset_id=dataset_id,\n",
    "                            stage=stage, status=status, reason=reason,\n",
    "                            partition_key=partition_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(DomoDataset)\n",
    "async def index_dataset(self: DomoDataset,\n",
    "                        auth: dmda.DomoAuth = None,\n",
    "                        dataset_id: str = None,\n",
    "                        debug_api: bool = False,\n",
    "                        session: httpx.AsyncClient = None\n",
    "                        ):\n",
    "\n",
    "    auth = auth or self.auth\n",
    "    dataset_id = dataset_id or self.id\n",
    "    return await dataset_routes.index_dataset(auth=auth, dataset_id=dataset_id, debug_api=debug_api,\n",
    "                                              session=session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(DomoDataset)\n",
    "async def upload_data(self : DomoDataset,\n",
    "                      upload_df: pd.DataFrame = None,\n",
    "                      upload_df_ls: list[pd.DataFrame] = None,\n",
    "                      upload_file: io.TextIOWrapper = None,\n",
    "\n",
    "                      upload_method: str = 'REPLACE',  # APPEND or REPLACE\n",
    "                      partition_key: str = None,\n",
    "\n",
    "                      is_index: bool = True,\n",
    "\n",
    "                      dataset_id: str = None,\n",
    "                      dataset_upload_id=None,\n",
    "\n",
    "                      auth: dmda.DomoAuth = None,\n",
    "\n",
    "                      session: httpx.AsyncClient = None,\n",
    "                      debug_api: bool = False,\n",
    "                      debug_prn: bool = False\n",
    "                      ):\n",
    "\n",
    "    auth, dataset_id = await _have_prereqs(self = self, auth = auth, dataset_id=dataset_id, function_name= \"upload_data\")\n",
    "\n",
    "    upload_df_ls = upload_df_ls or [upload_df]\n",
    "\n",
    "    status_message = f\"{dataset_id} {partition_key} | {auth.domo_instance}\"\n",
    "\n",
    "    # stage 1 get uploadId\n",
    "    if not dataset_upload_id:\n",
    "        if debug_prn:\n",
    "            print(f\"\\n\\n🎭 starting Stage 1 - {status_message}\")\n",
    "\n",
    "        stage_1_res = await dataset_routes.upload_dataset_stage_1(auth=auth,\n",
    "                                                                  dataset_id=dataset_id,\n",
    "                                                                  session=session,\n",
    "                                                                  partition_tag=partition_key,\n",
    "                                                                  debug_api=debug_api\n",
    "                                                                  )\n",
    "        if debug_prn:\n",
    "            print(f\"\\n\\n🎭 Stage 1 response -- {stage_1_res.status} for {status_message}\")\n",
    "\n",
    "        dataset_upload_id = stage_1_res.response.get('uploadId')\n",
    "\n",
    "    if not dataset_upload_id:\n",
    "        raise DomoDataset_UploadData_DatasetUploadId_Error(\n",
    "            domo_instance=auth.domo_instance,  dataset_id=dataset_id, stage=1, partition_key=partition_key,\n",
    "            status=stage_1_res.status, reason=stage_1_res.response)\n",
    "\n",
    "    # stage 2 upload_dataset\n",
    "    stage_2_res = None\n",
    "\n",
    "    if upload_file:\n",
    "        if debug_prn:\n",
    "            print(f\"\\n\\n🎭 starting Stage 2 - upload file for {status_message}\")\n",
    "\n",
    "        stage_2_res = await asyncio.gather(*[dataset_routes.upload_dataset_stage_2_file(auth=auth,\n",
    "                                                                                        dataset_id=dataset_id,\n",
    "                                                                                        upload_id=dataset_upload_id,\n",
    "                                                                                        part_id=1,\n",
    "                                                                                        data_file=upload_file,\n",
    "                                                                                        session=session, debug_api=debug_api)])\n",
    "\n",
    "    else:\n",
    "        if debug_prn:\n",
    "            print(\n",
    "                f\"\\n\\n🎭 starting Stage 2 - {len(upload_df_ls)} - number of parts for {status_message}\")\n",
    "        stage_2_res = await asyncio.gather(*[dataset_routes.upload_dataset_stage_2_df(auth=auth,\n",
    "                                                                                      dataset_id=dataset_id,\n",
    "                                                                                      upload_id=dataset_upload_id,\n",
    "                                                                                      part_id=index + 1,\n",
    "                                                                                      upload_df=df,\n",
    "                                                                                      session=session, debug_api=debug_api) for index, df in enumerate(upload_df_ls)])\n",
    "\n",
    "    for res in stage_2_res:\n",
    "        if not res.is_success:\n",
    "            raise DomoDataset_UploadData_UploadData_Error(\n",
    "                domo_instance=auth.domo_instance, dataset_id=dataset_id, stage=2, partition_key=partition_key,\n",
    "                status=res.status, reason=res.response)\n",
    "\n",
    "    if debug_prn:\n",
    "        print(f\"🎭 Stage 2 - upload data: complete for {status_message}\")\n",
    "\n",
    "    # stage 3 commit_data\n",
    "    if debug_prn:\n",
    "        print(f\"\\n\\n🎭 starting Stage 3 - commit dataset_upload_id for {status_message}\")\n",
    "\n",
    "    await asyncio.sleep(10)  # wait for uploads to finish\n",
    "    stage3_res = await dataset_routes.upload_dataset_stage_3(auth=auth,\n",
    "                                                             dataset_id=dataset_id,\n",
    "                                                             upload_id=dataset_upload_id,\n",
    "                                                             update_method=upload_method,\n",
    "                                                             partition_tag=partition_key,\n",
    "                                                             is_index=False,\n",
    "                                                             session=session,\n",
    "                                                             debug_api=debug_api)\n",
    "\n",
    "    if not stage3_res.is_success:\n",
    "        raise DomoDataset_UploadData_CommitDatasetUploadId_Error(\n",
    "            domo_instance=auth.domo_instance, dataset_id=dataset_id, partition_key=partition_key, stage=3,\n",
    "            status=stage3_res.status, reason=stage3_res.response)\n",
    "\n",
    "    if debug_prn:\n",
    "        print(f\"\\n🎭 stage 3 - commit dataset: complete for {status_message} \")\n",
    "\n",
    "    if is_index:\n",
    "        await asyncio.sleep(3)\n",
    "        return await self.index_dataset(auth=auth,\n",
    "                                        dataset_id=dataset_id,\n",
    "                                        debug_api=debug_api,\n",
    "                                        session=session)\n",
    "\n",
    "    return stage3_res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(DomoDataset)\n",
    "async def list_partitions(self : DomoDataset,\n",
    "                            auth: dmda.DomoAuth = None,\n",
    "                            dataset_id: str = None,\n",
    "                            debug_api: bool = False,\n",
    "                            session: httpx.AsyncClient = None\n",
    "                            ):\n",
    "\n",
    "    auth = auth or self.auth\n",
    "    dataset_id = dataset_id or self.id\n",
    "\n",
    "    res = await dataset_routes.list_partitions(auth=auth, dataset_id=dataset_id, debug_api=debug_api,\n",
    "                                                session=session)\n",
    "    if res.status != 200:\n",
    "        return None\n",
    "\n",
    "    return res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataId</th>\n",
       "      <th>partitionId</th>\n",
       "      <th>dateCompleted</th>\n",
       "      <th>rowCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>372</td>\n",
       "      <td>2013-07-02</td>\n",
       "      <td>2023-01-24T14:27:21.000+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>373</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>2023-01-24T14:27:21.000+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>354</td>\n",
       "      <td>2013-07-20</td>\n",
       "      <td>2023-01-24T14:27:20.000+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>355</td>\n",
       "      <td>2013-07-19</td>\n",
       "      <td>2023-01-24T14:27:20.000+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>356</td>\n",
       "      <td>2013-07-18</td>\n",
       "      <td>2023-01-24T14:27:20.000+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataId partitionId                  dateCompleted  rowCount\n",
       "0     372  2013-07-02  2023-01-24T14:27:21.000+00:00         1\n",
       "1     373  2013-07-01  2023-01-24T14:27:21.000+00:00         1\n",
       "2     354  2013-07-20  2023-01-24T14:27:20.000+00:00         1\n",
       "3     355  2013-07-19  2023-01-24T14:27:20.000+00:00         1\n",
       "4     356  2013-07-18  2023-01-24T14:27:20.000+00:00         1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "dataset_id = 'd2b21660-4ba8-400c-badf-aeef5a9abae1'\n",
    "\n",
    "ds = await DomoDataset.get_from_id(auth=token_auth, dataset_id=dataset_id)\n",
    "ds_partition_ls = await ds.list_partitions()\n",
    "\n",
    "pd.DataFrame(ds_partition_ls[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DomoDataset_CreateDataset_Error(Exception):\n",
    "    def __init__(self, domo_instance: str, dataset_name: str, status: int, reason: str):\n",
    "        message = f\"Failure to create dataset {dataset_name} in {domo_instance} :: {status} - {reason}\"\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "@patch_to(DomoDataset, cls_method=True)\n",
    "async def create(cls: DomoDataset,\n",
    "                 dataset_name: str,\n",
    "                 dataset_type='api',\n",
    "\n",
    "                 schema=None,\n",
    "                 auth: dmda.DomoAuth = None,\n",
    "                 debug_api: bool = False, \n",
    "                 session : httpx.AsyncClient = None\n",
    "                 ):\n",
    "    schema = schema or {\"columns\": [\n",
    "        {\"name\": 'col1', \"type\": 'LONG', \"upsertKey\": False},\n",
    "        {\"name\": 'col2', \"type\": 'STRING', \"upsertKey\": False}\n",
    "    ]}\n",
    "    \n",
    "\n",
    "    res = await dataset_routes.create(dataset_name=dataset_name,\n",
    "                                      dataset_type=dataset_type,\n",
    "                                      schema=schema, auth=auth, debug_api=debug_api, session=session\n",
    "                                      )\n",
    "\n",
    "    if not res.is_success:\n",
    "        raise DomoDataset_CreateDataset_Error(\n",
    "            domo_instance=auth.domo_instance, dataset_name=dataset_name, \n",
    "            status=res.status, reason=res.response)\n",
    "\n",
    "    dataset_id = res.response.get('dataSource').get('dataSourceId')\n",
    "\n",
    "    return await cls.get_from_id(dataset_id=dataset_id, auth=auth)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample implementation of create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "token_auth = dmda.DomoTokenAuth(\n",
    "    domo_instance=\"domo-community\", domo_access_token=os.environ[\"DOMO_DOJO_ACCESS_TOKEN\"]\n",
    ")\n",
    "\n",
    "# await DomoDataset.create( dataset_name= 'Hello world_v2', dataset_type='API', auth = token_auth, debug_api = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     async def delete_partition(self,\n",
    "#                                dataset_partition_id: str,\n",
    "\n",
    "#                                dataset_id: str = None,\n",
    "#                                empty_df: pd.DataFrame = None,\n",
    "\n",
    "#                                auth: DomoFullAuth = None,\n",
    "\n",
    "#                                is_index: bool = True,\n",
    "#                                debug_api: bool = False,\n",
    "#                                session: httpx.AsyncClient = None):\n",
    "\n",
    "#         is_close_session = True if not session else False\n",
    "\n",
    "#         session = session or httpx.AsyncClient()\n",
    "#         auth = auth or self.auth\n",
    "#         dataset_id = dataset_id or self.id\n",
    "\n",
    "# #        if empty_df is None:\n",
    "# #            empty_df = await self.query_dataset_private(auth=auth,\n",
    "# #                                                        dataset_id=dataset_id,\n",
    "# #                                                        sql=\"SELECT * from table limit 1\",\n",
    "# #                                                        debug=False)\n",
    "# #\n",
    "# #        await self.upload_csv(upload_df=empty_df.head(0),\n",
    "# #                              upload_method='REPLACE',\n",
    "# #                              is_index=is_index,\n",
    "# #                              partition_key=dataset_partition_id,\n",
    "# #                              session=session,\n",
    "# #                              debug=False)\n",
    "#         if debug_api:\n",
    "#             print(f\"\\n\\n🎭 starting Stage 1\")\n",
    "\n",
    "#         res = await dataset_routes.delete_partition_stage_1(auth=auth,\n",
    "#                                                             dataset_id=dataset_id,\n",
    "#                                                             dataset_partition_id=dataset_partition_id,\n",
    "#                                                             debug=debug, session=session)\n",
    "#         if debug_api:\n",
    "#             print(f\"\\n\\n🎭 Stage 1 response -- {res.status}\")\n",
    "#             print(res)\n",
    "\n",
    "#         stage_2_res = None\n",
    "#         if debug_api:\n",
    "#             print('starting Stage 2')\n",
    "#         stage_2_res = await dataset_routes.delete_partition_stage_2(auth=auth,\n",
    "#                                                                     dataset_id=dataset_id,\n",
    "#                                                                     dataset_partition_id=dataset_partition_id,\n",
    "#                                                                     debug=debug, session=session)\n",
    "#         if debug_api:\n",
    "#             print(f\"\\n\\n🎭 Stage 2 response -- {stage_2_res.status}\")\n",
    "\n",
    "#         stage_3_res = None\n",
    "#         if debug_api:\n",
    "#             print('starting Stage 3')\n",
    "#         stage_3_res = await dataset_routes.index_dataset(auth=auth,\n",
    "#                                                          dataset_id=dataset_id,\n",
    "#                                                          debug=debug, session=session)\n",
    "#         if debug_api:\n",
    "#             print(f\"\\n\\n🎭 Stage 3 response -- {stage_3_res.status}\")\n",
    "\n",
    "#         if is_close_session:\n",
    "#             await session.aclose()\n",
    "\n",
    "#         if debug_api:\n",
    "#             print(stage_3_res)\n",
    "\n",
    "#         if stage_3_res.status == 200:\n",
    "#             return res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     async def reset_dataset(self,\n",
    "#                             auth: DomoFullAuth = None,\n",
    "#                             is_index: bool = True,\n",
    "#                             debug_api: bool = False\n",
    "#                             ):\n",
    "#         execute_reset = input(\n",
    "#             \"This function will delete all rows.  Type BLOW_ME_AWAY to execute:\")\n",
    "\n",
    "#         if execute_reset != 'BLOW_ME_AWAY':\n",
    "#             print(\"You didn't type BLOW_ME_AWAY, moving on.\")\n",
    "#             return None\n",
    "\n",
    "#         auth = auth or self.auth\n",
    "#         dataset_id = self.id\n",
    "\n",
    "#         if not auth:\n",
    "#             raise Exception(\"auth required\")\n",
    "\n",
    "#         session = httpx.AsyncClient()\n",
    "\n",
    "#         # create empty dataset to retain schema\n",
    "#         empty_df = await self.query_dataset_private(auth=auth,\n",
    "#                                                     dataset_id=dataset_id,\n",
    "#                                                     sql=\"SELECT * from table limit 1\",\n",
    "#                                                     session=session,\n",
    "#                                                     debug=debug)\n",
    "#         empty_df = empty_df.head(0)\n",
    "\n",
    "#         # get partition list\n",
    "# #         partition_list = await dataset_routes.list_partitions(auth=auth,\n",
    "# #                                                               dataset_id=self.id,\n",
    "# #                                                               debug=debug,\n",
    "# #                                                               session=session)\n",
    "\n",
    "# #         if len(partition_list) > 0:\n",
    "# #             partition_list = chunk_list(partition_list, 100)\n",
    "\n",
    "# #             for index, pl in enumerate(partition_list):\n",
    "# #                 print(f'🥫 starting chunk {index + 1} of {len(partition_list)}')\n",
    "\n",
    "# #                 await asyncio.gather(*[self.delete_partition(auth=auth,\n",
    "# #                                                              dataset_partition_id=partition.get('partitionId'),\n",
    "# #                                                              session=session,\n",
    "# #                                                              empty_df=empty_df,\n",
    "# #                                                              debug=False) for partition in pl])\n",
    "# #                 if is_index:\n",
    "# #                     await self.index_dataset(session=session)\n",
    "\n",
    "#         res = await self.upload_csv(upload_df=empty_df,\n",
    "#                                     upload_method='REPLACE',\n",
    "#                                     is_index=is_index,\n",
    "#                                     session=session,\n",
    "#                                     debug=False)\n",
    "\n",
    "#         await session.aclose()\n",
    "#         return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
