{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp integrations.Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "import csv\n",
    "import datetime as DT\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import domolibrary.client.DomoAuth as dmda\n",
    "import domolibrary.classes.DomoInstanceConfig  as dmic\n",
    "import domolibrary.classes.DomoDataset as dmds\n",
    "\n",
    "\n",
    "import utils.Exceptions as ex\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LogError:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    function_str: str\n",
    "    message_str: str\n",
    "    domo_instance: str\n",
    "\n",
    "\n",
    "def write_error(file_path, log_err: LogError):\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    with open(file_path, 'a+') as log_file:\n",
    "        headers = list(log_err.__dict__.keys())\n",
    "        writer = csv.DictWriter(log_file, fieldnames=headers)\n",
    "\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerows([log_err.__dict__])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "async def get_ip_whitelist_config(auth: dmda.DomoFullAuth,\n",
    "                                  dataset_id: str,\n",
    "                                  handle_err_fn: callable,\n",
    "                                  sql: str = \"select addresses from table\",\n",
    "                                  debug_api: bool = False):\n",
    "    try:\n",
    "        sync_ip_ds = await dmds.DomoDataset.get_from_id(auth=auth,\n",
    "                                                        dataset_id=dataset_id,\n",
    "                                                        debug_api=debug_api)\n",
    "        if debug_api:\n",
    "            print(sync_ip_ds)\n",
    "\n",
    "        print(\n",
    "            f\"‚öôÔ∏è START - Retrieving whitelist configuration \\n{sync_ip_ds.display_url()}\")\n",
    "\n",
    "        sync_ip_df = await sync_ip_ds.query_dataset_private(auth=auth,\n",
    "                                                            dataset_id=dataset_id,\n",
    "                                                            loop_until_end = True,\n",
    "                                                            sql=sql)\n",
    "\n",
    "        if sync_ip_df.empty:\n",
    "            raise Exception('no whitelist returned')\n",
    "            return False\n",
    "\n",
    "        print(\n",
    "            f\"\\n‚öôÔ∏è SUCCESS üéâ Retrieved whitelist configuration  \\nThere are {len(sync_ip_df.index)} ip addresses to sync\")\n",
    "\n",
    "        return list(sync_ip_df['addresses'])\n",
    "\n",
    "    except ex.InvalidDataset:\n",
    "        print('invalid dataset')\n",
    "\n",
    "        handle_err_fn(log_err=LogError(function_str='get_ip_whitelist_config',\n",
    "                                       message_str=f'invalid dataset {dataset_id} not matched in {auth.domo_instance}',\n",
    "                                       domo_instance=auth.domo_instance))\n",
    "        return False\n",
    "\n",
    "    except Exception:\n",
    "        print(\"did it fail?\")\n",
    "        handle_err_fn(log_err=LogError(function_str='get_ip_whitelist_config',\n",
    "                                       message_str=f'undefined error',\n",
    "                                       domo_instance=auth.domo_instance))\n",
    "        return False\n",
    "\n",
    "\n",
    "async def remove_partition_by_x_days(auth: dmda.DomoFullAuth,\n",
    "                                     dataset_id: str,\n",
    "                                     x_last_days: int = 0,\n",
    "                                     separator: str = None,\n",
    "                                     date_index: int = 0,\n",
    "                                     date_format: str = '%Y-%m-%d'):\n",
    "    domo_ds = dmds.DomoDataset(auth=auth, id=dataset_id)\n",
    "\n",
    "    list_partition = await domo_ds.list_partitions(auth=auth, dataset_id=dataset_id)\n",
    "\n",
    "    today = DT.date.today()\n",
    "    days_ago = today - DT.timedelta(days=x_last_days)\n",
    "    for i in list_partition:\n",
    "        compare_date = ''\n",
    "        if separator is not None and separator != '':\n",
    "            compare_date = i['partitionId'].split(separator)[date_index]\n",
    "        else:\n",
    "            compare_date = i['partitionId']\n",
    "\n",
    "        try:\n",
    "            d = DT.datetime.strptime(compare_date, date_format).date()\n",
    "        except ValueError:\n",
    "            d = None\n",
    "        if d is not None and d < days_ago:\n",
    "            print(auth.domo_instance, ': üöÄ  Removing partition key : ',\n",
    "                  (i['partitionId']), ' in ', dataset_id)\n",
    "            await domo_ds.delete_partition(dataset_partition_id=i['partitionId'], dataset_id=dataset_id,\n",
    "                                           auth=auth)\n",
    "\n",
    "\n",
    "async def get_instance_whitelist_df(auth: dmda.DomoFullAuth\n",
    "                                        ) -> pd.DataFrame:\n",
    "    \"\"\"return a dataframe data in the correct shape for upload for ONE instance\"\"\"\n",
    "    instance_whitelist = await dmic.DomoInstanceConfig.get_whitelist(auth=auth)\n",
    "\n",
    "    if instance_whitelist == ['']:\n",
    "        instance_whitelist = ['no_ip_whitelist']\n",
    "\n",
    "    upload_df = pd.DataFrame(instance_whitelist, columns=['address'])\n",
    "    upload_df['instance'] = auth.domo_instance\n",
    "    upload_df['url'] = f'https://{auth.domo_instance}.domo.com/admin/security/whitelist'\n",
    "\n",
    "    return upload_df\n",
    "\n",
    "\n",
    "async def get_company_domains(auth: dmda.DomoFullAuth,\n",
    "                              dataset_id: str,\n",
    "                              handle_err_fn: callable,\n",
    "                              sql: str = \"select domain from table\",\n",
    "                              global_admin_username: str = None,\n",
    "                              global_admin_password: str = None,\n",
    "                              execution_env: str = None,\n",
    "                              debug_api: bool = False) -> pd.DataFrame:\n",
    "    ds = await dmds.DomoDataset.get_from_id(auth=auth,\n",
    "                                            id=dataset_id, debug_api=debug_api)\n",
    "\n",
    "    print(f\"‚öôÔ∏è START - Retrieving company list \\n{ds.display_url()}\")\n",
    "    print(f\"‚öôÔ∏è SQL = {sql}\")\n",
    "\n",
    "    df = await ds.query_dataset_private(auth=auth,\n",
    "                                        dataset_id=dataset_id,\n",
    "                                        sql=sql,\n",
    "                                        loop_until_end = True,\n",
    "                                        debug_api=debug_api)\n",
    "\n",
    "    df[\"domo_instance\"] = df[\"domain\"].apply(\n",
    "        lambda x: x.replace('.domo.com', ''))\n",
    "\n",
    "    if global_admin_username:\n",
    "        df[\"domo_username\"] = global_admin_username\n",
    "    if global_admin_password:\n",
    "        df[\"domo_password\"] = global_admin_password\n",
    "\n",
    "    if execution_env:\n",
    "        df['env'] = execution_env or 'manual'\n",
    "\n",
    "    if df.empty:\n",
    "        raise Exception('no companies retrieved')\n",
    "        return False\n",
    "\n",
    "    print(\n",
    "        f\"\\n‚öôÔ∏è SUCCESS üéâ Retrieved company list \\nThere are {len(df.index)} companies to update\")\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
