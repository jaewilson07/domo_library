{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: utils.upload_data.html\n",
    "title: Upload Data to Domo\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils.upload_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "import httpx\n",
    "\n",
    "import domolibrary.client.Logger as lc\n",
    "import domolibrary.client.DomoAuth as dmda\n",
    "import domolibrary.classes.DomoDataset as dmds\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import os\n",
    "import domolibrary.classes.DomoBootstrap as dmbsr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "async def loop_upload(\n",
    "    upload_df: pd.DataFrame,\n",
    "    consol_ds: dmds.DomoDataset,\n",
    "    partition_key: str,\n",
    "    upload_method: str,\n",
    "    logger: lc.Logger,\n",
    "    debug_api: bool = False,\n",
    "    debug_prn: bool = False,\n",
    "    debug_fn: bool = True,\n",
    "    max_retry: int = 2,\n",
    "    is_index: bool = False,\n",
    "):\n",
    "    base_msg = (\n",
    "        f\"{partition_key} in {consol_ds.auth.domo_instance}\"\n",
    "        if partition_key\n",
    "        else f\"in {consol_ds.auth.domo_instance}\"\n",
    "    )\n",
    "\n",
    "    if debug_fn:\n",
    "        print(\n",
    "            f\"starting upload of {len(upload_df)} rows to {base_msg} with {max_retry} attempts\"\n",
    "        )\n",
    "\n",
    "    retry_attempt = 1\n",
    "\n",
    "    res = None\n",
    "\n",
    "    while retry_attempt <= max_retry and not res:\n",
    "        try:\n",
    "            if debug_fn:\n",
    "                print(f\"attempt {retry_attempt}/{max_retry} for {base_msg}\")\n",
    "\n",
    "            res = await consol_ds.upload_data(\n",
    "                upload_df=upload_df,\n",
    "                upload_method=\"REPLACE\" if partition_key else upload_method,\n",
    "                partition_key=partition_key,\n",
    "                is_index=is_index,\n",
    "                debug_api=debug_api,\n",
    "                debug_prn=debug_prn,\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            retry_attempt += 1\n",
    "\n",
    "            message = f\"âš ï¸ upload_data : unexpected error: {e} in {partition_key} during retry_attempt {retry_attempt}/{max_retry}\"\n",
    "            logger.log_warning(message)\n",
    "            if debug_fn:\n",
    "                print(message)\n",
    "\n",
    "    if not res:\n",
    "        raise Exception(\n",
    "            f\"ðŸ’£ failed to upload data {len(upload_df)} rows to {base_msg} - {retry_attempt}/{max_retry} retries reached\"\n",
    "        )\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "async def upload_data(\n",
    "    # instance where the data_fn function will execute against\n",
    "    data_fn,  # data function to execute\n",
    "    instance_auth: dmda.DomoAuth,  # instance to run the data function against\n",
    "    consol_ds: dmds.DomoDataset,  # dataset where data should be accumulated\n",
    "    # if partition key supplied, will replace existing partition\n",
    "    partition_key: str = None,\n",
    "    upload_method: str = \"REPLACE\",\n",
    "    is_index: bool = False,  # index dataset\n",
    "    debug_prn: bool = False,\n",
    "    debug_fn: bool = True,\n",
    "    debug_api: bool = False,\n",
    "    logger: lc.Logger = None,\n",
    "    max_retry: int = 2,  # number of times to attempt upload\n",
    "):\n",
    "    logger = logger or lc.Logger(app_name=\"upload_data\")\n",
    "\n",
    "    try:\n",
    "        message = f\"ðŸ starting {instance_auth.domo_instance} - {data_fn.__name__}\"\n",
    "        logger.log_info(message)\n",
    "        print(message)\n",
    "\n",
    "        instance_session = httpx.AsyncClient()\n",
    "\n",
    "        upload_df = await data_fn(instance_auth, instance_session, debug_api=debug_api)\n",
    "\n",
    "        if upload_df is None or (\n",
    "            isinstance(upload_df, pd.DataFrame) and len(upload_df.index) == 0\n",
    "        ):\n",
    "            message = f\"no data to upload for {partition_key}: {consol_ds.id} in {consol_ds.auth.domo_instance}\"\n",
    "            logger.log_info(message)\n",
    "            print(message)\n",
    "            return None\n",
    "\n",
    "        if debug_prn:\n",
    "            print(upload_df[0:5])\n",
    "\n",
    "        res = await loop_upload(\n",
    "            upload_df=upload_df,\n",
    "            consol_ds=consol_ds,\n",
    "            partition_key=partition_key,\n",
    "            upload_method=upload_method,\n",
    "            debug_api=debug_api,\n",
    "            debug_prn=debug_prn,\n",
    "            debug_fn=debug_fn,\n",
    "            max_retry=max_retry,\n",
    "            logger=logger,\n",
    "            is_index=False,\n",
    "        )\n",
    "\n",
    "        if res.is_success:\n",
    "            message = f\"ðŸš€ success upload of {partition_key} to {consol_ds.id} in {consol_ds.auth.domo_instance} in {data_fn.__name__}\"\n",
    "            logger.log_info(message)\n",
    "\n",
    "        else:\n",
    "            message = f\"ðŸ’£ upload_data successful status but failed to upload {partition_key} - {res.status} - {res.response} in {data_fn.__name__}\"\n",
    "            logger.log_error(message)\n",
    "\n",
    "        print(message)\n",
    "\n",
    "        return res\n",
    "\n",
    "    finally:\n",
    "        if is_index:\n",
    "\n",
    "            res = await consol_ds.index_dataset(\n",
    "                debug_api=debug_api, session=instance_session\n",
    "            )\n",
    "            if res.is_success:\n",
    "                message = f\"ðŸ¥« successfully indexed {consol_ds.name} in {consol_ds.auth.domo_instance}\"\n",
    "                logger.log_info(message)\n",
    "            else:\n",
    "                message = f\"ðŸ’€âš ï¸ failure to index {consol_ds.name} in {consol_ds.auth.domo_instance}\"\n",
    "                logger.log_error(message)\n",
    "\n",
    "            print(message)\n",
    "\n",
    "        await instance_session.aclose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample implementation of upload_data with loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusting num_stacks_to_drop, consider revising `get_traceback` call\n",
      "{'stack_length': 18, 'module_index': 12, 'num_stacks_to_drop_passed': 5}\n",
      "ðŸ starting domo-community - data_fn\n",
      "starting upload of 419 rows to domo-community in domo-community with 2 attempts\n",
      "attempt 1/2 for domo-community in domo-community\n",
      "adjusting num_stacks_to_drop, consider revising `get_traceback` call\n",
      "{'stack_length': 18, 'module_index': 12, 'num_stacks_to_drop_passed': 5}\n",
      "ðŸš€ success upload of domo-community to 44c5af30-ea04-49e4-9d7a-529afd223590 in domo-community in data_fn\n",
      "adjusting num_stacks_to_drop, consider revising `get_traceback` call\n",
      "{'stack_length': 18, 'module_index': 12, 'num_stacks_to_drop_passed': 5}\n",
      "ðŸ¥« successfully indexed demo_instance_features in domo-community\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResponseGetData(status=200, response={'dataSourceId': '44c5af30-ea04-49e4-9d7a-529afd223590', 'uploadId': 1421, 'dataTag': 'domo-community', 'status': 'SUCCESS', 'size': {'rowCount': 419, 'columnCount': 7, 'numberOfBytes': 25956, 'partCount': 1}, 'indexing': {'requested': False}}, is_success=True, parent_class=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | eval : false\n",
    "\n",
    "# import domolibrary.classes.DomoBootstrap as dmbsr\n",
    "# import httpx\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "async def data_fn(\n",
    "    instance_auth: dmda.DomoFullAuth,  # this API requires full auth\n",
    "    session: httpx.AsyncClient = None,\n",
    "    debug_api: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"function to call.  must return a dataframe.\"\"\"\n",
    "    try:\n",
    "        bsr = dmbsr.DomoBootstrap(auth=instance_auth)\n",
    "        instance_features = await bsr.get_features(debug_api=debug_api, session=session)\n",
    "\n",
    "        upload_df = pd.DataFrame(instance_features)\n",
    "        upload_df[\"instance\"] = instance_auth.domo_instance\n",
    "\n",
    "        return upload_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"getting data : unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "### get_auth\n",
    "full_auth = dmda.DomoFullAuth(\n",
    "    domo_instance=\"domo-community\",\n",
    "    domo_username=os.environ[\"DOMO_USERNAME\"],\n",
    "    domo_password=os.environ[\"DOJO_PASSWORD\"],\n",
    ")\n",
    "\n",
    "# confirm retrieves token\n",
    "assert isinstance(await full_auth.get_auth_token(), str)\n",
    "\n",
    "ds_id = \"44c5af30-ea04-49e4-9d7a-529afd223590\"\n",
    "ds = await dmds.DomoDataset.get_from_id(dataset_id=ds_id, auth=full_auth)\n",
    "\n",
    "await upload_data(\n",
    "    instance_auth=full_auth,  # instance where the data_fn function will execute against\n",
    "    consol_ds=ds,\n",
    "    partition_key=full_auth.domo_instance,\n",
    "    data_fn=data_fn,\n",
    "    is_index=True,\n",
    "    debug_fn=True,\n",
    "    debug_api=False,\n",
    "    max_retry=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | export\n",
    "# async def upload_data_with_date(\n",
    "#     instance_auth,\n",
    "#     data_fn,\n",
    "#     consol_ds,\n",
    "#     partition_date_col,\n",
    "#     partition_delimiter,\n",
    "#     start_date,\n",
    "#     end_date,\n",
    "#     debug_api: bool = False,\n",
    "#     debug_prn: bool = False,\n",
    "# ):\n",
    "\n",
    "#     instance_session = httpx.AsyncClient()\n",
    "\n",
    "#     print(\n",
    "#         f\"'ðŸŽ¬ upload_with_data: starting retrieval {start_date}, {end_date}, {instance_auth.domo_instance}\"\n",
    "#     )\n",
    "\n",
    "#     upload_df = await data_fn(\n",
    "#         instance_auth=instance_auth,\n",
    "#         session=instance_session,\n",
    "#         start_date=start_date,\n",
    "#         end_date=end_date,\n",
    "#         debug=debug,\n",
    "#     )\n",
    "\n",
    "#     await instance_session.aclose()\n",
    "\n",
    "#     if not isinstance(upload_df, pd.DataFrame):\n",
    "#         print(f\"ðŸ›‘ error no data returned {instance_auth.domo_instance}\")\n",
    "#         print(upload_df)\n",
    "#         return None\n",
    "\n",
    "#     if debug_prn:\n",
    "#         print(\n",
    "#             f\"ðŸ§» upload_with_data: starting upload {len(upload_df)} rows for {instance_auth.domo_instance}\"\n",
    "#         )\n",
    "\n",
    "#     task = []\n",
    "\n",
    "#     for index, partition_set in upload_df.drop_duplicates(\n",
    "#         subset=[partition_date_col]\n",
    "#     ).iterrows():\n",
    "#         partition_date = partition_set[partition_date_col]\n",
    "\n",
    "#         partition_key = (\n",
    "#             f\"{instance_auth.domo_instance}{partition_delimiter}{str(partition_date)}\"\n",
    "#         )\n",
    "\n",
    "#         task.append(\n",
    "#             consol_ds.upload_data(\n",
    "#                 upload_df=upload_df[(upload_df[partition_date_col] == partition_date)],\n",
    "#                 upload_method=\"REPLACE\",\n",
    "#                 partition_key=partition_key,\n",
    "#                 is_index=False,\n",
    "#                 debug_api=debug_api,\n",
    "#                 debug_prn=debug_prn,\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     res = await asyncio.gather(*task)\n",
    "\n",
    "#     if debug_prn:\n",
    "#         print(\n",
    "#             f\"ðŸŽ‰ upload_with_data : finished uploading {len(upload_df)} rows for {instance_auth.domo_instance}\"\n",
    "#         )\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
